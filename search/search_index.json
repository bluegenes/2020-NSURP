{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NSURP Research Project 2020 The materials in this repository are designed to facilitate learning bioinformatic techniques while working through a metagenomics project using publicly-available data. If you see a mistake or something is not clear, please submit an issue . During this project, you will learn how to: keep a detailed lab notebook interact with an HPC (we'll use Farm ) install and manage software environments using conda download sequencing data and other files from the internet and public databases interpret and use different file formats in bioinformatics and computing conduct quality analysis and control for sequencing data determine the taxonomic composition of sequencing reads quickly compare large sequencing datasets build reproducible workflows using snakemake document workflows using git and GitHub troubleshoot errors during your analysis Most of the work done in this rotation will be completed on Farm. However, you will need to access Farm from your own computer. We will use an SSH-client to be able to interact with Farm. If you are using a Mac or running Linux, your computer comes with a program (e.g. Terminal on Mac) that we can use as an SSH-client. If you are on a Windows running Windows 10, you can install the Ubuntu Subsystem . Otherwise, please follow the instructions for Windows found at this link .","title":"Home"},{"location":"#nsurp-research-project-2020","text":"The materials in this repository are designed to facilitate learning bioinformatic techniques while working through a metagenomics project using publicly-available data. If you see a mistake or something is not clear, please submit an issue . During this project, you will learn how to: keep a detailed lab notebook interact with an HPC (we'll use Farm ) install and manage software environments using conda download sequencing data and other files from the internet and public databases interpret and use different file formats in bioinformatics and computing conduct quality analysis and control for sequencing data determine the taxonomic composition of sequencing reads quickly compare large sequencing datasets build reproducible workflows using snakemake document workflows using git and GitHub troubleshoot errors during your analysis Most of the work done in this rotation will be completed on Farm. However, you will need to access Farm from your own computer. We will use an SSH-client to be able to interact with Farm. If you are using a Mac or running Linux, your computer comes with a program (e.g. Terminal on Mac) that we can use as an SSH-client. If you are on a Windows running Windows 10, you can install the Ubuntu Subsystem . Otherwise, please follow the instructions for Windows found at this link .","title":"NSURP Research Project 2020"},{"location":"00.getting-started/","text":"NSURP Research Project 2020 This repository contains step-by-step resources for running a metagenomics workflow. If you see a mistake or something is not clear, please submit an issue . During this project, you will learn how to: + interact with an HPC (we'll use Farm ) + install and manage software environments using conda + download sequencing data and other files from the internet and public databases + interpret and use different file formats in bioinformatics and computing + quality analysis and control for sequencing data + determine the taxonomic composition of sequencing reads + assemble and annotate metagenomic reads + quickly compare large sequencing datasets + document workflows using git and GitHub. The files in this repository are ordered by execution, meaning file 00* should be completed before 01* . Most of the work done in this rotation will be completed on Farm. However, you will need to access Farm from your own computer. We will use an SSH-client to be able to interact with Farm. If you are using a Mac or a Linux, your computer comes with a program called Terminal that we will use as an SSH-client. If you are on a Windows running Windows 10, you can install the Ubuntu Subsystem . Otherwise, please follow the instructions for Windows found at this link .","title":"NSURP Research Project 2020"},{"location":"00.getting-started/#nsurp-research-project-2020","text":"This repository contains step-by-step resources for running a metagenomics workflow. If you see a mistake or something is not clear, please submit an issue . During this project, you will learn how to: + interact with an HPC (we'll use Farm ) + install and manage software environments using conda + download sequencing data and other files from the internet and public databases + interpret and use different file formats in bioinformatics and computing + quality analysis and control for sequencing data + determine the taxonomic composition of sequencing reads + assemble and annotate metagenomic reads + quickly compare large sequencing datasets + document workflows using git and GitHub. The files in this repository are ordered by execution, meaning file 00* should be completed before 01* . Most of the work done in this rotation will be completed on Farm. However, you will need to access Farm from your own computer. We will use an SSH-client to be able to interact with Farm. If you are using a Mac or a Linux, your computer comes with a program called Terminal that we will use as an SSH-client. If you are on a Windows running Windows 10, you can install the Ubuntu Subsystem . Otherwise, please follow the instructions for Windows found at this link .","title":"NSURP Research Project 2020"},{"location":"01.using-farm/","text":"Getting Started on the Farm HPC High Performance Computing (HPC) refers to computers that have more capability than a typical personal computer (i.e. most desktops and laptops). Many research problems we encounter when analyzing sequencing data require more resources than we have available on our laptops. For this, we use large, remote compute systems that have more resources available. Most universities have access to an HPC (or cluster) that has a large amount of hard drive space to store files, RAM for computing tasks, and CPUs for processing. Other options for accessing large computers include NSF XSEDE services like Jetstream and paid services like Amazon Web Services or Google Cloud. We will use the UC Davis Farm Cluster during this rotation. Getting an account on Farm To be able to use Farm, you need to sign up for an account. Farm requires key file authentication. Key files come in pairs like the locks and keys on doors. The private key file is the first file, and it is like the key to a door. This file is private and should never be shared with anyone (do not post this file on GitHub, slack, etc.). The public key file is the second file, and it is like the lock on a door. It is publicly viewable, but cannot be \"unlocked\" without the private key file. We need to generate a key file pair in order to create a farm account. Open the Terminal application or the Terminal emulator you installed in the first_lesson . Change directories into the .ssh folder. This folder is where key file pairs are typically stored. cd ~/.ssh If this command does not work, create your own ssh folder and cd into it: mkdir -p ~/.ssh cd ~/.ssh Then, generate the keyfile pair by running: ssh-keygen Follow the prompts on the screen. If prompted for a password, you can hit Enter on your keyboard to avoid setting one. Two files will be created by this command. These files should have the same prefix. The file that ends in .pub is the public key. The account request form Next, navigate to this page . From the first drop down menu (Which cluster are you applying for an account on?), select FARM/CAES . From the second drop down menu (Who is sponsoring your account?), select Brown, C. Titus . Then, upload your public key file to the page. Submit the form. If the cluster admins and Titus approve your account, you will now have farm access! Don't loose the key file pair you just made. You will need the private key file each time you log into farm. Connecting to a remote computer Once you have a farm account, we will use the command ssh to connect to farm. ssh stands for \"secure shell\". To connect to your account on farm, type: ssh -i ~/.ssh/your_keyfile_name username@farm.cse.ucdavis.edu If you are successful, you will see a message that looks something like this: Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-70-generic x86_64) 1 updates could not be installed automatically. For more details, see /var/log/unattended-upgrades/unattended-upgrades.log *** System restart required *** A transfer node, c11-42, is available for rsync, scp, gzip From outside the Farm cluster use port 2022 to access the transfer node. ssh -p 2022 username@farm.cse.ucdavis.edu scp -P 2022 src username@farm.cse.ucdavis.edu:/destination REMINDER: Farm does not back up user data. Please ensure your data is backed up offsite. *** Dec 04 2019: * 2:10pm - Service restored. Please report any issues to help@cse.ucdavis.edu. Email help@cse.ucdavis.edu for help with Farm. Downtime scheduled for the first Wednesday of Oct and April. The next downtime is Wednesday April 1st at 11:59pm. If interested in contributing to farm, the rates for 5 years are: $ 1,000 per 10TB, served from redundant servers with compression $ 8,800 per parallel node (256GB ram, 32 cores/64 threads, 2TB /scratch) $17,500 per GPU node (Nvidia Telsa V100, dual Xeon 4114, 2TB /scratch) $22,700 per bigmem node (1TB ram, 48 cores/96 threads, 2TB /scratch) Last login: Thu Jan 2 17:01:36 2020 from 76.105.143.194 Module slurm/19.05.3 loaded Module openmpi/4.0.1 loaded username@farm:~$ When you first login to farm, you will be in your home directory. This is where you will write your files and run the majority of your commands. When you are done using farm, you can exit your ssh connection with the exit command. exit","title":"Setting up Farm"},{"location":"01.using-farm/#getting-started-on-the-farm-hpc","text":"High Performance Computing (HPC) refers to computers that have more capability than a typical personal computer (i.e. most desktops and laptops). Many research problems we encounter when analyzing sequencing data require more resources than we have available on our laptops. For this, we use large, remote compute systems that have more resources available. Most universities have access to an HPC (or cluster) that has a large amount of hard drive space to store files, RAM for computing tasks, and CPUs for processing. Other options for accessing large computers include NSF XSEDE services like Jetstream and paid services like Amazon Web Services or Google Cloud. We will use the UC Davis Farm Cluster during this rotation.","title":"Getting Started on the Farm HPC"},{"location":"01.using-farm/#getting-an-account-on-farm","text":"To be able to use Farm, you need to sign up for an account. Farm requires key file authentication. Key files come in pairs like the locks and keys on doors. The private key file is the first file, and it is like the key to a door. This file is private and should never be shared with anyone (do not post this file on GitHub, slack, etc.). The public key file is the second file, and it is like the lock on a door. It is publicly viewable, but cannot be \"unlocked\" without the private key file. We need to generate a key file pair in order to create a farm account. Open the Terminal application or the Terminal emulator you installed in the first_lesson . Change directories into the .ssh folder. This folder is where key file pairs are typically stored. cd ~/.ssh If this command does not work, create your own ssh folder and cd into it: mkdir -p ~/.ssh cd ~/.ssh Then, generate the keyfile pair by running: ssh-keygen Follow the prompts on the screen. If prompted for a password, you can hit Enter on your keyboard to avoid setting one. Two files will be created by this command. These files should have the same prefix. The file that ends in .pub is the public key.","title":"Getting an account on Farm"},{"location":"01.using-farm/#the-account-request-form","text":"Next, navigate to this page . From the first drop down menu (Which cluster are you applying for an account on?), select FARM/CAES . From the second drop down menu (Who is sponsoring your account?), select Brown, C. Titus . Then, upload your public key file to the page. Submit the form. If the cluster admins and Titus approve your account, you will now have farm access! Don't loose the key file pair you just made. You will need the private key file each time you log into farm.","title":"The account request form"},{"location":"01.using-farm/#connecting-to-a-remote-computer","text":"Once you have a farm account, we will use the command ssh to connect to farm. ssh stands for \"secure shell\". To connect to your account on farm, type: ssh -i ~/.ssh/your_keyfile_name username@farm.cse.ucdavis.edu If you are successful, you will see a message that looks something like this: Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-70-generic x86_64) 1 updates could not be installed automatically. For more details, see /var/log/unattended-upgrades/unattended-upgrades.log *** System restart required *** A transfer node, c11-42, is available for rsync, scp, gzip From outside the Farm cluster use port 2022 to access the transfer node. ssh -p 2022 username@farm.cse.ucdavis.edu scp -P 2022 src username@farm.cse.ucdavis.edu:/destination REMINDER: Farm does not back up user data. Please ensure your data is backed up offsite. *** Dec 04 2019: * 2:10pm - Service restored. Please report any issues to help@cse.ucdavis.edu. Email help@cse.ucdavis.edu for help with Farm. Downtime scheduled for the first Wednesday of Oct and April. The next downtime is Wednesday April 1st at 11:59pm. If interested in contributing to farm, the rates for 5 years are: $ 1,000 per 10TB, served from redundant servers with compression $ 8,800 per parallel node (256GB ram, 32 cores/64 threads, 2TB /scratch) $17,500 per GPU node (Nvidia Telsa V100, dual Xeon 4114, 2TB /scratch) $22,700 per bigmem node (1TB ram, 48 cores/96 threads, 2TB /scratch) Last login: Thu Jan 2 17:01:36 2020 from 76.105.143.194 Module slurm/19.05.3 loaded Module openmpi/4.0.1 loaded username@farm:~$ When you first login to farm, you will be in your home directory. This is where you will write your files and run the majority of your commands. When you are done using farm, you can exit your ssh connection with the exit command. exit","title":"Connecting to a remote computer"},{"location":"02.conda/","text":"Using Conda for Software Installation This section covers using conda to install scientific software. What is Conda? Installing scientific software (including all required dependencies of said software!) is often challenging. Conda is a software manager that helps you find and install software packages. Set up Miniconda To get started, we'll install miniconda, which contains everything we need to get started with conda. Log in to farm and run the following commands to install Miniconda. Follow the prompts on the screen and accept all default options. Install conda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Again, be sure to answer yes to any yes/no questions, especially the last question about conda init ! This will ensure conda is fully installed and you'll be able to use it for the commands below. Copy .bashrc code to .bash_profile FARM runs .bash_profile on startup (not .bashrc ). Here, we explicitly run the .bashrc from the .bash_profile file echo source ~/.bashrc >> ~/.bash_profile Activate conda Miniconda is now installed, but we need to activate it to be able to use it. source ~/.bashrc This command executes our ~/.bashrc command, which should now have You should now see (base) in front of your prompt, indicating that you are in the base environment. Optional: Add colors to your terminal output If you have preferred settings for bash, go ahead and set them up. Open the .bash_profile file using nano cd nano .bash_profile Now, add the following to the document: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$\" export CLICOLOR=1 export LSCOLORS=ExFxBxDxCxegedabagacad alias ls='ls --color=auto' And close and save the document ( Ctrl-X , Y , <Enter> ) Now source the file: source .bash_profile Try an ls ! Is your miniconda3 folder a different color than the Miniconda3-latest-Linux-x86_64.sh installer? Configuring channels Conda works by searching for software packages in online repositories ( Channels ). By default, conda searches for software only in Continuum\u2019s (Conda\u2019s developer) channels. Most of the scientific software we'll be using is not available within the default channel, so we will add additional channels to our conda settings. Channels in Conda are ordered. The channel with the highest priority is the first one that Conda checks, looking for the package you asked for. You can change this order, and also add channels to it (and set their priority as well). If multiple channels contain a package, and one channel contains a newer version than the other one, the order of the channels\u2019 determines which one of these two versions are going to be installed, even if the higher priority channel contains the older version. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Note that these commands stack. In this case, the highest priority channel will be conda-forge , followed by bioconda and then the defaults channel. So when installing software, conda will start by looking for our desired software packing in the conda-forge channel, then search in the bioconda channel, and finally search in the defaults channel. Using Conda environments Different software packages often have different \"dependencies\": other software packages that are required for installation. In many cases, you'll need software with dependencies that conflict -- e.g. one program requires python version 3, while the other requires python 2. To avoid conflicts, we install software into \"environments\" that are isolated from one another - so that the software installed in one environment does not impact the software installed in another environment. Create an environment Let's start by creating an environment for this project conda create -y --name nsurp-env This creates an empty environment named nsurp-env . To activate this environment, run: conda activate nsurp-env Your prompt should now start with (nsurp-env) . Install software into the environment We can now install software into our environment. Let's install sourmash, which we will use in a later lesson. conda install -y sourmash Deactivating and Exiting When you'd like to leave your environment, you can type conda deactivate and you will return to the base environment. When you log out of farm by typing exit , when you end a tmux or screen session, or when an srun job ends, your environment will automatically be deactivated. To restart the environment, you can run conda activate nsurp-env . Additional Resources This tutorial covers the basics of conda including a brief introduction to conda and why it is useful, installation and setup, creating environments, and installing software. These videos cover the material in the above tutorial: video 1 video 2 (there were some technical issues with this recording...sorry!)","title":"Install Conda"},{"location":"02.conda/#using-conda-for-software-installation","text":"This section covers using conda to install scientific software.","title":"Using Conda for Software Installation"},{"location":"02.conda/#what-is-conda","text":"Installing scientific software (including all required dependencies of said software!) is often challenging. Conda is a software manager that helps you find and install software packages.","title":"What is Conda?"},{"location":"02.conda/#set-up-miniconda","text":"To get started, we'll install miniconda, which contains everything we need to get started with conda. Log in to farm and run the following commands to install Miniconda. Follow the prompts on the screen and accept all default options.","title":"Set up Miniconda"},{"location":"02.conda/#install-conda","text":"wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Again, be sure to answer yes to any yes/no questions, especially the last question about conda init ! This will ensure conda is fully installed and you'll be able to use it for the commands below.","title":"Install conda"},{"location":"02.conda/#copy-bashrc-code-to-bash_profile","text":"FARM runs .bash_profile on startup (not .bashrc ). Here, we explicitly run the .bashrc from the .bash_profile file echo source ~/.bashrc >> ~/.bash_profile","title":"Copy .bashrc code to .bash_profile"},{"location":"02.conda/#activate-conda","text":"Miniconda is now installed, but we need to activate it to be able to use it. source ~/.bashrc This command executes our ~/.bashrc command, which should now have You should now see (base) in front of your prompt, indicating that you are in the base environment.","title":"Activate conda"},{"location":"02.conda/#optional-add-colors-to-your-terminal-output","text":"If you have preferred settings for bash, go ahead and set them up. Open the .bash_profile file using nano cd nano .bash_profile Now, add the following to the document: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$\" export CLICOLOR=1 export LSCOLORS=ExFxBxDxCxegedabagacad alias ls='ls --color=auto' And close and save the document ( Ctrl-X , Y , <Enter> ) Now source the file: source .bash_profile Try an ls ! Is your miniconda3 folder a different color than the Miniconda3-latest-Linux-x86_64.sh installer?","title":"Optional: Add colors to your terminal output"},{"location":"02.conda/#configuring-channels","text":"Conda works by searching for software packages in online repositories ( Channels ). By default, conda searches for software only in Continuum\u2019s (Conda\u2019s developer) channels. Most of the scientific software we'll be using is not available within the default channel, so we will add additional channels to our conda settings. Channels in Conda are ordered. The channel with the highest priority is the first one that Conda checks, looking for the package you asked for. You can change this order, and also add channels to it (and set their priority as well). If multiple channels contain a package, and one channel contains a newer version than the other one, the order of the channels\u2019 determines which one of these two versions are going to be installed, even if the higher priority channel contains the older version. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Note that these commands stack. In this case, the highest priority channel will be conda-forge , followed by bioconda and then the defaults channel. So when installing software, conda will start by looking for our desired software packing in the conda-forge channel, then search in the bioconda channel, and finally search in the defaults channel.","title":"Configuring channels"},{"location":"02.conda/#using-conda-environments","text":"Different software packages often have different \"dependencies\": other software packages that are required for installation. In many cases, you'll need software with dependencies that conflict -- e.g. one program requires python version 3, while the other requires python 2. To avoid conflicts, we install software into \"environments\" that are isolated from one another - so that the software installed in one environment does not impact the software installed in another environment.","title":"Using Conda environments"},{"location":"02.conda/#create-an-environment","text":"Let's start by creating an environment for this project conda create -y --name nsurp-env This creates an empty environment named nsurp-env . To activate this environment, run: conda activate nsurp-env Your prompt should now start with (nsurp-env) .","title":"Create an environment"},{"location":"02.conda/#install-software-into-the-environment","text":"We can now install software into our environment. Let's install sourmash, which we will use in a later lesson. conda install -y sourmash","title":"Install software into the environment"},{"location":"02.conda/#deactivating-and-exiting","text":"When you'd like to leave your environment, you can type conda deactivate and you will return to the base environment. When you log out of farm by typing exit , when you end a tmux or screen session, or when an srun job ends, your environment will automatically be deactivated. To restart the environment, you can run conda activate nsurp-env .","title":"Deactivating and Exiting"},{"location":"02.conda/#additional-resources","text":"This tutorial covers the basics of conda including a brief introduction to conda and why it is useful, installation and setup, creating environments, and installing software. These videos cover the material in the above tutorial: video 1 video 2 (there were some technical issues with this recording...sorry!)","title":"Additional Resources"},{"location":"03.lab-notebook/","text":"Keeping a Lab Notebook Just like with wetlab work, it's important to document everything you do during a computational workflow. Where to take notes For this project, we recommend using HackMD . HackMD works a little like google docs, but enables better formatting for adding code. You can start with just regular text, but as you start adding screnshots, code blocks, and header sections, you can use Markdown syntax to improve formatting of your rendered notes. HackMD shows you a few examples of Markdown when you first open a new document, but you can also check out this markdown tutorial if you want to learn more. How to take notes It's ok to provide the minimum amount of information necessary to execute a set of commands (i.e., you don't necessary have to record every failure, every ls , etc), but it is important to document each step. + Copying and pasting the commands that worked is a great way to record them (the history command can be helpful to see what you've run in the past). Documentation is for you! (And also for others) Your lab notebook and documentation is most useful for future you. Keep in mind that things that seem super obvious right now will likely be forgetten within a few weeks/months. Try to be detailed enough so that if you tried to pick up this project again in 3 months (or 3 years), you would be able to understand exactly what to do and how to do it. With a good lab notebook, you can save yourself from troubleshooting the same errors over and over again, as well as greatly simplify the process of writing up your Materials and Methods section for any reports or papers. Finally, good lab notebooks help keep everyone working on your project (both now and in the future) on the same page. Other systems for taking notes: After this project, if you like HackMD, great! Stick with it. If not: Using google docs or Microsoft Word for documenting computer commands can be hard because of autocorrection. We generally recommend against using these programs. Using a plain text editor (Notepad, Notepad++, Atom, BBEdit, TextEdit, nano, vim) avoids autocorrect problems but still has a nice user interface. Jupyter Lab is very useful for interactive research explorations and notetaking. We'll try this out in a later section. Eventually, we'll work through using git and GitHub to record and version control our workflows, but for now it's enough to write down everything you do.","title":"Keeping a Lab Notebook"},{"location":"03.lab-notebook/#keeping-a-lab-notebook","text":"Just like with wetlab work, it's important to document everything you do during a computational workflow.","title":"Keeping a Lab Notebook"},{"location":"03.lab-notebook/#where-to-take-notes","text":"For this project, we recommend using HackMD . HackMD works a little like google docs, but enables better formatting for adding code. You can start with just regular text, but as you start adding screnshots, code blocks, and header sections, you can use Markdown syntax to improve formatting of your rendered notes. HackMD shows you a few examples of Markdown when you first open a new document, but you can also check out this markdown tutorial if you want to learn more.","title":"Where to take notes"},{"location":"03.lab-notebook/#how-to-take-notes","text":"It's ok to provide the minimum amount of information necessary to execute a set of commands (i.e., you don't necessary have to record every failure, every ls , etc), but it is important to document each step. + Copying and pasting the commands that worked is a great way to record them (the history command can be helpful to see what you've run in the past).","title":"How to take notes"},{"location":"03.lab-notebook/#documentation-is-for-you-and-also-for-others","text":"Your lab notebook and documentation is most useful for future you. Keep in mind that things that seem super obvious right now will likely be forgetten within a few weeks/months. Try to be detailed enough so that if you tried to pick up this project again in 3 months (or 3 years), you would be able to understand exactly what to do and how to do it. With a good lab notebook, you can save yourself from troubleshooting the same errors over and over again, as well as greatly simplify the process of writing up your Materials and Methods section for any reports or papers. Finally, good lab notebooks help keep everyone working on your project (both now and in the future) on the same page.","title":"Documentation is for you! (And also for others)"},{"location":"03.lab-notebook/#other-systems-for-taking-notes","text":"After this project, if you like HackMD, great! Stick with it. If not: Using google docs or Microsoft Word for documenting computer commands can be hard because of autocorrection. We generally recommend against using these programs. Using a plain text editor (Notepad, Notepad++, Atom, BBEdit, TextEdit, nano, vim) avoids autocorrect problems but still has a nice user interface. Jupyter Lab is very useful for interactive research explorations and notetaking. We'll try this out in a later section. Eventually, we'll work through using git and GitHub to record and version control our workflows, but for now it's enough to write down everything you do.","title":"Other systems for taking notes:"},{"location":"04.sourmash-tutorial/","text":"Getting started with Sourmash: a tutorial Let's go through a sourmash tutorial . Commands (as of 07/31/2020) reproduced here. If doing this at a later date and these commands don't work, run the tutorial using the link above instead!! Making signatures, comparing, and searching 7/31/20 You'll need about 5 GB of free disk space, and about 5 GB of RAM to search GenBank. First activate your conda envionment: conda activate tutorial Because we installed sourmash into our tutorial environment, you should now be able to use the sourmash command: sourmash info Download the data and put it in a folder Lets make some folders to keep our project organized: mkdir smtut cd smtut mkdir data cd data Now we can download some data into our data folder wget https://s3.amazonaws.com/public.ged.msu.edu/ecoli_ref-5m.fastq.gz wget https://s3.amazonaws.com/public.ged.msu.edu/ecoliMG1655.fa.gz OR wget https://bit.ly/2CXj13R -O ecoli_ref-5m.fastq.gz wget https://bit.ly/2PdRbCJ -O ecoliMG1655.fa.gz The file that ends in .fastq.gz is a zipped file with DNA sequences in fastq format Lets take a look at our data: first we can unzip the fastq file, gunzip --keep ecoli_ref-5m.fastq.gz and use ls -lh to compare the sizes the zipped file should be smaller than the unzipped file we can look at the unzipped file using less ecoli_ref-5m.fastq it will have this fastq format: Computing a sourmash signature Compute a scaled signature from our reads: first, lets make a folder to keep our signatures in mkdir ~/smtut/sigs cd ~/smtut/sigs Compare reads to assemblies how much of the read content is contained in the reference genome? Build a signature for the E. coli reads with sourmash compute , sourmash compute \\ --scaled 10000 \\ ~/smtut/data/ecoli_ref*.fastq.gz \\ -o ~/smtut/sigs/ecoli-reads.sig \\ -k 31 Next build a signature for the E. coli genome with sourmash compute , sourmash compute \\ --scaled 10000 \\ ~/smtut/data/ecoliMG1655.fa.gz \\ -o ~/smtut/sigs/ecoli-genome.sig \\ -k 31 this command will call the software to make a kmer signature, keep only 1 in 10000 kmers from the sequence, store the signature where we ask it to, and use a k size of 31 Now evaluate containment , that is, what fraction of the read content is contained in the genome: sourmash search -k 31 ecoli-reads.sig ecoli-genome.sig --containment and you should see: # running sourmash subcommand: search loaded query: /home/ubuntu/data/ecoli_ref-5m... (k=31, DNA) loaded 1 signatures from ecoli-genome.sig 1 matches: similarity match ---------- ----- 10.6% /home/ubuntu/data/ecoliMG1655.fa.gz Try the reverse - why is it bigger? sourmash search -k 31 ecoli-genome.sig ecoli-reads.sig --containment","title":"Try Sourmash!"},{"location":"04.sourmash-tutorial/#getting-started-with-sourmash-a-tutorial","text":"Let's go through a sourmash tutorial . Commands (as of 07/31/2020) reproduced here. If doing this at a later date and these commands don't work, run the tutorial using the link above instead!!","title":"Getting started with Sourmash: a tutorial"},{"location":"04.sourmash-tutorial/#making-signatures-comparing-and-searching","text":"7/31/20 You'll need about 5 GB of free disk space, and about 5 GB of RAM to search GenBank. First activate your conda envionment: conda activate tutorial Because we installed sourmash into our tutorial environment, you should now be able to use the sourmash command: sourmash info","title":"Making signatures, comparing, and searching"},{"location":"04.sourmash-tutorial/#download-the-data-and-put-it-in-a-folder","text":"Lets make some folders to keep our project organized: mkdir smtut cd smtut mkdir data cd data Now we can download some data into our data folder wget https://s3.amazonaws.com/public.ged.msu.edu/ecoli_ref-5m.fastq.gz wget https://s3.amazonaws.com/public.ged.msu.edu/ecoliMG1655.fa.gz OR wget https://bit.ly/2CXj13R -O ecoli_ref-5m.fastq.gz wget https://bit.ly/2PdRbCJ -O ecoliMG1655.fa.gz The file that ends in .fastq.gz is a zipped file with DNA sequences in fastq format Lets take a look at our data: first we can unzip the fastq file, gunzip --keep ecoli_ref-5m.fastq.gz and use ls -lh to compare the sizes the zipped file should be smaller than the unzipped file we can look at the unzipped file using less ecoli_ref-5m.fastq it will have this fastq format:","title":"Download the data and put it in a folder"},{"location":"04.sourmash-tutorial/#computing-a-sourmash-signature","text":"Compute a scaled signature from our reads: first, lets make a folder to keep our signatures in mkdir ~/smtut/sigs cd ~/smtut/sigs","title":"Computing a sourmash signature"},{"location":"04.sourmash-tutorial/#compare-reads-to-assemblies","text":"how much of the read content is contained in the reference genome? Build a signature for the E. coli reads with sourmash compute , sourmash compute \\ --scaled 10000 \\ ~/smtut/data/ecoli_ref*.fastq.gz \\ -o ~/smtut/sigs/ecoli-reads.sig \\ -k 31 Next build a signature for the E. coli genome with sourmash compute , sourmash compute \\ --scaled 10000 \\ ~/smtut/data/ecoliMG1655.fa.gz \\ -o ~/smtut/sigs/ecoli-genome.sig \\ -k 31 this command will call the software to make a kmer signature, keep only 1 in 10000 kmers from the sequence, store the signature where we ask it to, and use a k size of 31 Now evaluate containment , that is, what fraction of the read content is contained in the genome: sourmash search -k 31 ecoli-reads.sig ecoli-genome.sig --containment and you should see: # running sourmash subcommand: search loaded query: /home/ubuntu/data/ecoli_ref-5m... (k=31, DNA) loaded 1 signatures from ecoli-genome.sig 1 matches: similarity match ---------- ----- 10.6% /home/ubuntu/data/ecoliMG1655.fa.gz Try the reverse - why is it bigger? sourmash search -k 31 ecoli-genome.sig ecoli-reads.sig --containment","title":"Compare reads to assemblies"},{"location":"05.starting-a-work-session/","text":"Starting a Work Session on FARM Any time you log onto FARM to work on this project, follow these steps to get access to computing resources. 1. Enter a tmux session This command creates a new tmux session: tmux new -s nsurp Note: If you already created this session, and want to re-join it, use tmux attach instead. 2. Get access to a compute node When you log on to our FARM computing system, you'll be on a login node, which is basically a computer with very few resources. These login nodes are shared among all users on farm. If we run any computing on these login nodes, logging into and navigating farm will slow down for everyone else! Instead, the moment that we want to do anything substantial, we want to ask farm for a more capable comptuter. Farm uses a \"job scheduler\" to make sure everyone gets access to the computational resources that they need. We can use the following command to get access to a computer that will fit our needs: srun -p bmm -J nsurp-analysis -t 5:00:00 --mem=10G --pty bash srun uses the computer's job scheduler SLURM to allocate you a computer -p specifies the job queue we want to use, and is specific to our farm accounts. -J nsurp-analysis is the \"job name\" assigned to this session. It can be modified to give your session a more descriptive name, e.g. -J download-data -t denotes that we want the computer for that amount of time (in this case, 3 hours). --mem specifies the amount of memory we'd like the computer to have. Here we've asked for 10 Gigabytes (10G). --pty bash specified that we want the linux shell to be the bash shell, which is the standard shell we've been working wiht so far Note that your home directory (the files you see) will be the same for both the login node and the computer you get access to. This is because both read and write from the same hard drives. So you can create files while in an srun session, and they'll still be there for you when you logout. 3. Activate your Conda Environment Once you're in an srun session, activate your project environment to get access to the software you've installed conda activate nsurp-env Leaving your tmux session Exit tmux by Ctrl-b , d Reattaching to your tmux session tmux attach Note: if you make more than one tmux session, you can see all session names by typing tmux ls , and then attaching to the right one with tmux attach -t <NAME>","title":"Starting a Work Session"},{"location":"05.starting-a-work-session/#starting-a-work-session-on-farm","text":"Any time you log onto FARM to work on this project, follow these steps to get access to computing resources.","title":"Starting a Work Session on FARM"},{"location":"05.starting-a-work-session/#1-enter-a-tmux-session","text":"This command creates a new tmux session: tmux new -s nsurp Note: If you already created this session, and want to re-join it, use tmux attach instead.","title":"1. Enter a tmux session"},{"location":"05.starting-a-work-session/#2-get-access-to-a-compute-node","text":"When you log on to our FARM computing system, you'll be on a login node, which is basically a computer with very few resources. These login nodes are shared among all users on farm. If we run any computing on these login nodes, logging into and navigating farm will slow down for everyone else! Instead, the moment that we want to do anything substantial, we want to ask farm for a more capable comptuter. Farm uses a \"job scheduler\" to make sure everyone gets access to the computational resources that they need. We can use the following command to get access to a computer that will fit our needs: srun -p bmm -J nsurp-analysis -t 5:00:00 --mem=10G --pty bash srun uses the computer's job scheduler SLURM to allocate you a computer -p specifies the job queue we want to use, and is specific to our farm accounts. -J nsurp-analysis is the \"job name\" assigned to this session. It can be modified to give your session a more descriptive name, e.g. -J download-data -t denotes that we want the computer for that amount of time (in this case, 3 hours). --mem specifies the amount of memory we'd like the computer to have. Here we've asked for 10 Gigabytes (10G). --pty bash specified that we want the linux shell to be the bash shell, which is the standard shell we've been working wiht so far Note that your home directory (the files you see) will be the same for both the login node and the computer you get access to. This is because both read and write from the same hard drives. So you can create files while in an srun session, and they'll still be there for you when you logout.","title":"2. Get access to a compute node"},{"location":"05.starting-a-work-session/#3-activate-your-conda-environment","text":"Once you're in an srun session, activate your project environment to get access to the software you've installed conda activate nsurp-env","title":"3. Activate your Conda Environment"},{"location":"05.starting-a-work-session/#leaving-your-tmux-session","text":"Exit tmux by Ctrl-b , d","title":"Leaving your tmux session"},{"location":"05.starting-a-work-session/#reattaching-to-your-tmux-session","text":"tmux attach Note: if you make more than one tmux session, you can see all session names by typing tmux ls , and then attaching to the right one with tmux attach -t <NAME>","title":"Reattaching to your tmux session"},{"location":"06.download-assess-ibd-data/","text":"Download and Visually Assess the Data Metagenomics is the analysis of genetic material from environmental samples (\"environment\" here meaning anything from human gut to open ocean). Metagenomics (DNA sequencing) and metatranscriptomics (RNA sequencing) can be used to assess the composition and functional potential of microbial commmunities. Human-associated microbial communities, such as the trillions of microorganisms that colonize the human gut, have co-evolved with humans and play important roles both in human biology and disease. Gut symbionts contribute to human digestive and metabolic functions, immune system regulation, and regulation of the intestinal epithelial barrier, including providing protection against pathogens. Inflammatory bowel disease (IBD) is an umbrella term used for diseases (Crohn's disease, Ulcerative Colitis) characterized by chronic inflammation of the intestines. These diseases impact about 3 million people in the United States. IBD is thought to be caused by a combination of genetic and environmental factors that alter gut homestasis and trigger immune-mediated inflammation. In particular, IBD is associated with an alteration of the composition of gut microbiota (\"dysbiosis\"), though the exact impact of the microbial community is still under investigation. Here, we will compare metagenome samples from patients with Inflammatory bowel disease (IBD) to samples from patients without IBD. We will characterize the microbial community associated with IBD vs non-IBD and assess the results in the context of current community findings for IBD Background Reading Here are some articles that contain good background info on the human microbiome and IBD. The human microbiome in evolution Host\u2013microbiota interactions in inflammatory bowel disease Microbial genes and pathways in inflammatory bowel disease Using FARM for downloads and analysis: Follow the instructions on Starting a Work Session on FARM to start a tmux session, get access to a compute node (via an srun interactive session). Download the data Now that we have a computer, let's download the data. note that you can also run these steps (and most analyses) on your personal computer Make a project directory cd mkdir -p 2020-NSURP/raw_data download samples to raw_data directory cd 2020-NSURP/raw_data Now, download two files for each of the following sample accession numbers using wget : # patient with Crohns disease wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/CSM7KOJO.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/CSM7KOJG.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/CSM7KOJE.tar # patient with no IBD wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSM5MD5B.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSM5MD5D.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSM6XRSX.tar if you do ls now, you should see the following: CSM7KOJE.tar CSM7KOJO.tar HSM5MD5D.tar CSM7KOJG.tar HSM5MD5B.tar HSM6XRSX.tar Untar each set of files: tar xf CSM7KOJO.tar Now, let's make the files difficult to modify or delete: chmod u-w *fastq.gz FASTQ format Although it looks complicated (and it is), we can understand the fastq format with a little decoding. Some rules about the format include... Line Description 1 Always begins with '@' and then information about the read 2 The actual DNA sequence 3 Always begins with a '+' and sometimes the same info in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 We can view the first complete read in a fastq file by using head to look at the first four lines. Because the our files are gzipped, we first temporarily decompress them with zcat . zcat CSM7KOJE_R1.fastq.gz | head -n 4 The first four lines of the file look something like this: Note: this is a different dataset, so your will look slightly different, though the formatting is the same @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCC@CAYNHANXX170426:3:1101:10002:54478/1 ATCCTTTACAATTACAAGATGCGTATGACCGCCTGATACAACAAGACATAAGAACGGAGGTTCCAGCTCTAAGTTTTCTATATAATTGGCAGAAATATG + B<BBBFFFFFFFFFFFFFFFFFBFFFFFFFFFFF<BFBFFFFFFF<FFFFFBFFFF/<F/BFF<<BFFBFFFFFFF<F<</FF/FFFFBFF</<FB7FFTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### Line 4 shows the quality for each nucleotide in the read. Quality is interpreted as the probability of an incorrect base call (e.g. 1 in 10) or, equivalently, the base call accuracy (e.g. 90%). To make it possible to line up each individual nucleotide with its quality score, the numerical score is converted into a code where each individual character represents the numerical quality score for an individual nucleotide. 'For example, in the line above, the quality score line is: CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### The numerical value assigned to each of these characters depends on the sequencing platform that generated the reads. The sequencing machine used to generate our data uses the standard Sanger quality PHRED score encoding, using Illumina version 1.8 onwards. Each character is assigned a quality score between 0 and 41 as shown in the chart below. Quality encoding: !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJ | | | | | Quality score: 01........11........21........31........41 Each quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based, so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%. These probability values are the results from the base calling algorithm and depend on how much signal was captured for the base incorporation. Looking back at our example read: @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCCTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### we can now see that there is a range of quality scores, but that the end of the sequence is very poor ( # = a quality score of 2). How does the first read in SRR1211680_1.fastq.gz compare to this example? Assessing Quality with FastQC For the most part, you won't be assessing the quality of all your reads by visually inspecting your FASTQ files. Rather, you'll be using a software program to assess read quality and filter out poor quality reads. We'll first use a program called FastQC to visualize the quality of our reads. FastQC has a number of features which can give you a quick impression of any problems your data may have, so you can take these issues into consideration before moving forward with your analyses. Rather than looking at quality scores for each individual read, FastQC looks at quality collectively across all reads within a sample. The image below shows one FastQC-generated plot that indicatesa very high quality sample: The x-axis displays the base position in the read, and the y-axis shows quality scores. In this example, the sample contains reads that are 40 bp long. This is much shorter than the reads we are working with in our workflow. For each position, there is a box-and-whisker plot showing the distribution of quality scores for all reads at that position. The horizontal red line indicates the median quality score and the yellow box shows the 1st to 3rd quartile range. This means that 50% of reads have a quality score that falls within the range of the yellow box at that position. The whiskers show the absolute range, which covers the lowest (0th quartile) to highest (4th quartile) values. For each position in this sample, the quality values do not drop much lower than 32. This is a high quality score. The plot background is also color-coded to identify good (green), acceptable (yellow), and bad (red) quality scores. Now let's take a look at a quality plot on the other end of the spectrum. Here, we see positions within the read in which the boxes span a much wider range. Also, quality scores drop quite low into the \"bad\" range, particularly on the tail end of the reads. The FastQC tool produces several other diagnostic plots to assess sample quality, in addition to the one plotted above. Running FastQC We will now assess the quality of the reads that we downloaded. First, make sure you're still in the raw_data directory cd ~/2020-NSURP/raw_data Next, activate the conda environment we created in the Install Conda lesson. conda activate nsurp-env Now, use conda to install fastqc. conda install fastqc FastQC can accept multiple file names as input, and on both zipped and unzipped files, so we can use the *.fastq* wildcard to run FastQC on all of the FASTQ files in this directory. fastqc *.fastq* The FastQC program has created several new files within our directory. For each input FASTQ file, FastQC has created a .zip file and a .html file. The .zip file extension indicates that this is actually a compressed set of multiple output files. We'll be working with these output files soon. The .html file is a stable webpage displaying the summary report for each of our samples. Transferring data from Farm to your computer To transfer a file from a remote server to our own machines, we will use scp . To learn more about scp , see the bottom of this tutorial . Now we can transfer our HTML files to our local computer using scp . The ./ indicates that you're transferring files to the directory you're currently working from. scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020-NSURP/raw_data/*.html ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the FastQC output. Decoding the FastQC Output We've now looked at quite a few \"Per base sequence quality\" FastQC graphs, but there are nine other graphs that we haven't talked about! Below we have provided a brief overview of interpretations for each of these plots. For more information, please see the FastQC documentation here Per tile sequence quality : the machines that perform sequencing are divided into tiles. This plot displays patterns in base quality along these tiles. Consistently low scores are often found around the edges, but hot spots can also occur in the middle if an air bubble was introduced at some point during the run. Per sequence quality scores : a density plot of quality for all reads at all positions. This plot shows what quality scores are most common. Per base sequence content : plots the proportion of each base position over all of the reads. Typically, we expect to see each base roughly 25% of the time at each position, but this often fails at the beginning or end of the read due to quality or adapter content. Per sequence GC content : a density plot of average GC content in each of the reads. Per base N content : the percent of times that 'N' occurs at a position in all reads. If there is an increase at a particular position, this might indicate that something went wrong during sequencing. Sequence Length Distribution : the distribution of sequence lengths of all reads in the file. If the data is raw, there is often on sharp peak, however if the reads have been trimmed, there may be a distribution of shorter lengths. Sequence Duplication Levels : A distribution of duplicated sequences. In sequencing, we expect most reads to only occur once. If some sequences are occurring more than once, it might indicate enrichment bias (e.g. from PCR). If the samples are high coverage (or RNA-seq or amplicon), this might not be true. Overrepresented sequences : A list of sequences that occur more frequently than would be expected by chance. Adapter Content : a graph indicating where adapater sequences occur in the reads. K-mer Content : a graph showing any sequences which may show a positional bias within the reads. Extra Info if you ever need to download >10 accessions from the SRA, the sra-toolkit is a great tool to do this with! However, we find sra-toolkit cumbersome when only a couple accessions need to be downloaded.","title":"Download and Visually Assess"},{"location":"06.download-assess-ibd-data/#download-and-visually-assess-the-data","text":"Metagenomics is the analysis of genetic material from environmental samples (\"environment\" here meaning anything from human gut to open ocean). Metagenomics (DNA sequencing) and metatranscriptomics (RNA sequencing) can be used to assess the composition and functional potential of microbial commmunities. Human-associated microbial communities, such as the trillions of microorganisms that colonize the human gut, have co-evolved with humans and play important roles both in human biology and disease. Gut symbionts contribute to human digestive and metabolic functions, immune system regulation, and regulation of the intestinal epithelial barrier, including providing protection against pathogens. Inflammatory bowel disease (IBD) is an umbrella term used for diseases (Crohn's disease, Ulcerative Colitis) characterized by chronic inflammation of the intestines. These diseases impact about 3 million people in the United States. IBD is thought to be caused by a combination of genetic and environmental factors that alter gut homestasis and trigger immune-mediated inflammation. In particular, IBD is associated with an alteration of the composition of gut microbiota (\"dysbiosis\"), though the exact impact of the microbial community is still under investigation. Here, we will compare metagenome samples from patients with Inflammatory bowel disease (IBD) to samples from patients without IBD. We will characterize the microbial community associated with IBD vs non-IBD and assess the results in the context of current community findings for IBD","title":"Download and Visually Assess the Data"},{"location":"06.download-assess-ibd-data/#background-reading","text":"Here are some articles that contain good background info on the human microbiome and IBD. The human microbiome in evolution Host\u2013microbiota interactions in inflammatory bowel disease Microbial genes and pathways in inflammatory bowel disease","title":"Background Reading"},{"location":"06.download-assess-ibd-data/#using-farm-for-downloads-and-analysis","text":"Follow the instructions on Starting a Work Session on FARM to start a tmux session, get access to a compute node (via an srun interactive session).","title":"Using FARM for downloads and analysis:"},{"location":"06.download-assess-ibd-data/#download-the-data","text":"Now that we have a computer, let's download the data. note that you can also run these steps (and most analyses) on your personal computer Make a project directory cd mkdir -p 2020-NSURP/raw_data download samples to raw_data directory cd 2020-NSURP/raw_data Now, download two files for each of the following sample accession numbers using wget : # patient with Crohns disease wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/CSM7KOJO.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/CSM7KOJG.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/CSM7KOJE.tar # patient with no IBD wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSM5MD5B.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSM5MD5D.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSM6XRSX.tar if you do ls now, you should see the following: CSM7KOJE.tar CSM7KOJO.tar HSM5MD5D.tar CSM7KOJG.tar HSM5MD5B.tar HSM6XRSX.tar Untar each set of files: tar xf CSM7KOJO.tar Now, let's make the files difficult to modify or delete: chmod u-w *fastq.gz","title":"Download the data"},{"location":"06.download-assess-ibd-data/#fastq-format","text":"Although it looks complicated (and it is), we can understand the fastq format with a little decoding. Some rules about the format include... Line Description 1 Always begins with '@' and then information about the read 2 The actual DNA sequence 3 Always begins with a '+' and sometimes the same info in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 We can view the first complete read in a fastq file by using head to look at the first four lines. Because the our files are gzipped, we first temporarily decompress them with zcat . zcat CSM7KOJE_R1.fastq.gz | head -n 4 The first four lines of the file look something like this: Note: this is a different dataset, so your will look slightly different, though the formatting is the same @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCC@CAYNHANXX170426:3:1101:10002:54478/1 ATCCTTTACAATTACAAGATGCGTATGACCGCCTGATACAACAAGACATAAGAACGGAGGTTCCAGCTCTAAGTTTTCTATATAATTGGCAGAAATATG + B<BBBFFFFFFFFFFFFFFFFFBFFFFFFFFFFF<BFBFFFFFFF<FFFFFBFFFF/<F/BFF<<BFFBFFFFFFF<F<</FF/FFFFBFF</<FB7FFTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### Line 4 shows the quality for each nucleotide in the read. Quality is interpreted as the probability of an incorrect base call (e.g. 1 in 10) or, equivalently, the base call accuracy (e.g. 90%). To make it possible to line up each individual nucleotide with its quality score, the numerical score is converted into a code where each individual character represents the numerical quality score for an individual nucleotide. 'For example, in the line above, the quality score line is: CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### The numerical value assigned to each of these characters depends on the sequencing platform that generated the reads. The sequencing machine used to generate our data uses the standard Sanger quality PHRED score encoding, using Illumina version 1.8 onwards. Each character is assigned a quality score between 0 and 41 as shown in the chart below. Quality encoding: !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJ | | | | | Quality score: 01........11........21........31........41 Each quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based, so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%. These probability values are the results from the base calling algorithm and depend on how much signal was captured for the base incorporation. Looking back at our example read: @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCCTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### we can now see that there is a range of quality scores, but that the end of the sequence is very poor ( # = a quality score of 2). How does the first read in SRR1211680_1.fastq.gz compare to this example?","title":"FASTQ format"},{"location":"06.download-assess-ibd-data/#assessing-quality-with-fastqc","text":"For the most part, you won't be assessing the quality of all your reads by visually inspecting your FASTQ files. Rather, you'll be using a software program to assess read quality and filter out poor quality reads. We'll first use a program called FastQC to visualize the quality of our reads. FastQC has a number of features which can give you a quick impression of any problems your data may have, so you can take these issues into consideration before moving forward with your analyses. Rather than looking at quality scores for each individual read, FastQC looks at quality collectively across all reads within a sample. The image below shows one FastQC-generated plot that indicatesa very high quality sample: The x-axis displays the base position in the read, and the y-axis shows quality scores. In this example, the sample contains reads that are 40 bp long. This is much shorter than the reads we are working with in our workflow. For each position, there is a box-and-whisker plot showing the distribution of quality scores for all reads at that position. The horizontal red line indicates the median quality score and the yellow box shows the 1st to 3rd quartile range. This means that 50% of reads have a quality score that falls within the range of the yellow box at that position. The whiskers show the absolute range, which covers the lowest (0th quartile) to highest (4th quartile) values. For each position in this sample, the quality values do not drop much lower than 32. This is a high quality score. The plot background is also color-coded to identify good (green), acceptable (yellow), and bad (red) quality scores. Now let's take a look at a quality plot on the other end of the spectrum. Here, we see positions within the read in which the boxes span a much wider range. Also, quality scores drop quite low into the \"bad\" range, particularly on the tail end of the reads. The FastQC tool produces several other diagnostic plots to assess sample quality, in addition to the one plotted above.","title":"Assessing Quality with FastQC"},{"location":"06.download-assess-ibd-data/#running-fastqc","text":"We will now assess the quality of the reads that we downloaded. First, make sure you're still in the raw_data directory cd ~/2020-NSURP/raw_data Next, activate the conda environment we created in the Install Conda lesson. conda activate nsurp-env Now, use conda to install fastqc. conda install fastqc FastQC can accept multiple file names as input, and on both zipped and unzipped files, so we can use the *.fastq* wildcard to run FastQC on all of the FASTQ files in this directory. fastqc *.fastq* The FastQC program has created several new files within our directory. For each input FASTQ file, FastQC has created a .zip file and a .html file. The .zip file extension indicates that this is actually a compressed set of multiple output files. We'll be working with these output files soon. The .html file is a stable webpage displaying the summary report for each of our samples.","title":"Running FastQC"},{"location":"06.download-assess-ibd-data/#transferring-data-from-farm-to-your-computer","text":"To transfer a file from a remote server to our own machines, we will use scp . To learn more about scp , see the bottom of this tutorial . Now we can transfer our HTML files to our local computer using scp . The ./ indicates that you're transferring files to the directory you're currently working from. scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020-NSURP/raw_data/*.html ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the FastQC output.","title":"Transferring data from Farm to your computer"},{"location":"06.download-assess-ibd-data/#decoding-the-fastqc-output","text":"We've now looked at quite a few \"Per base sequence quality\" FastQC graphs, but there are nine other graphs that we haven't talked about! Below we have provided a brief overview of interpretations for each of these plots. For more information, please see the FastQC documentation here Per tile sequence quality : the machines that perform sequencing are divided into tiles. This plot displays patterns in base quality along these tiles. Consistently low scores are often found around the edges, but hot spots can also occur in the middle if an air bubble was introduced at some point during the run. Per sequence quality scores : a density plot of quality for all reads at all positions. This plot shows what quality scores are most common. Per base sequence content : plots the proportion of each base position over all of the reads. Typically, we expect to see each base roughly 25% of the time at each position, but this often fails at the beginning or end of the read due to quality or adapter content. Per sequence GC content : a density plot of average GC content in each of the reads. Per base N content : the percent of times that 'N' occurs at a position in all reads. If there is an increase at a particular position, this might indicate that something went wrong during sequencing. Sequence Length Distribution : the distribution of sequence lengths of all reads in the file. If the data is raw, there is often on sharp peak, however if the reads have been trimmed, there may be a distribution of shorter lengths. Sequence Duplication Levels : A distribution of duplicated sequences. In sequencing, we expect most reads to only occur once. If some sequences are occurring more than once, it might indicate enrichment bias (e.g. from PCR). If the samples are high coverage (or RNA-seq or amplicon), this might not be true. Overrepresented sequences : A list of sequences that occur more frequently than would be expected by chance. Adapter Content : a graph indicating where adapater sequences occur in the reads. K-mer Content : a graph showing any sequences which may show a positional bias within the reads.","title":"Decoding the FastQC Output"},{"location":"06.download-assess-ibd-data/#extra-info","text":"if you ever need to download >10 accessions from the SRA, the sra-toolkit is a great tool to do this with! However, we find sra-toolkit cumbersome when only a couple accessions need to be downloaded.","title":"Extra Info"},{"location":"07.quality-control/","text":"Quality Control the Data If you're starting a new work session on FARM, be sure to follow the instructions here . After downloading sequencing data , the next step in many pipelines is to perform quality control trimming on the reads. However, deciding when and how to trim data is pipeline dependent. Below, we define a few types of quality control and explore a use cases and how trimming recommendations may change with different applications. Although this project focuses on metagenomic sequencing, we include other applications in this discussion. Types of Quality Control Adapter and barcode trimming : Adapter sequences are added to a sample library to aid in the physical process of sequencing. They are ubiquitous within a certain chemistry, and so are present across all sequenced samples. Barcodes are unique nucleotide sequences used to identify a specific sample when multiple samples are sequenced in a single lane. After barcoded samples are separated from one another in a process called demultiplexing, barcodes are no longer needed in a sequence. It is generally a good idea to remove adapters and barcodes from sequencing samples before proceeding with any downstream application. However, if you are using a pipeline that involves matching between reads and a quality reference, you may get similar results with or without adapter trimming. For quick estimation Quality trimming : Quality trimming removes low-quality bases from sequences reads. The user can set the stringency cut off for \"low quality\" by indicating a phred score at which to trim. K-mer trimming : K-mer trimming removes k-mers that occur very few times in a sequencing dataset. In reads with sufficient sequencing depth, we expect real k-mers to occur multiple times. When a single sequencing error occurs in a read, this produces k erroneous k-mers. K-mer trimming trims a read to remove all of these k-mers. K-mer trimming does not rely on information from the sequencer like phred scores, but instead on the biological signal in the reads themselves. When and how to trim? Trimming is a balance of removing artificial or incorrect nucleotides and retaining true nucleotides in sequencing data. What and when to trim therefore changes with the sequencing application, and with the sequencing data itself. Below we explore some trimming use cases to help develop an intuition for what type of trimming is necessary and when. Single-species genomic sequencing for assembly : Let's imagine we have just sequenced an Escherichia coli isolate with 100X coverage and would like to assemble the isolate. We would first want to remove adapters and barcodes to prevent these sequences from ending up in our final assembly. Then, stringent quality and k-mer trimming may be appropriate, because we have high coverage data; even if we were to stringently trim and were only left with 50% of our original number of reads, we would still have 50X coverage of very high quality data. 50X coverage is sufficient to acheive a good bacterial assembly in most cases. de novo RNA-sequencing assembly Now let's imagine we have sequenced the transcriptome of our favorite species which does not currently have a reference transcriptome. Because RNA transcripts have different abundance profiles, we can't use average coverage in the same way as we used it for single-species genomic sequencing. We need to be more careful when we k-mer and error trim so as not to accidentally remove low-abundance reads that represent true transcripts. We would likely use light quality trimming (e.g. a phred score of ~5). For k-mer trimming, we would only trim reads that contain high-abundance k-mers. Metagenome de novo assembly Trimming metagenomic reads for de novo assembly is similar to trimming RNA-sequencing reads for de novo transcriptome assembly. Because there are often low-abundance organisms that have low-coverage in our sequencing datasets, we need to be careful not to accidently remove these during trimming. Metagenome read mapping In referenced-based analyses including mapping of metagenomic reads to a set of reference genomes, reads will often map even when they contain adapters and barcodes. However, in some cases, the presence of adapters and barcodes does prevent mapping, so it is safer to remove all barcodes and adapters. References about trimming Many scientific studies have explored the trimming parameter space in an effort to make recommendations for different applications. We include some of these studies below. On the optimal trimming of high-throughput mRNA sequence data An Extensive Evaluation of Read Trimming Effects on Illumina NGS Data Analysis Quality and Adapter trimming with Fastp We will use fastp to do quality trimming of our reads. In the Download and Visual Assessment Module , we saw using FastQC that the Illumina Universal Adapter was present in our samples. We also saw that the sequence read quality dropped dramatically toward the end of the read. We will remove both of these sequences using fastp. Fastp also creates its own FastQC-style html reports for the files that we can look at after running. Run fastp Reminder, make sure you've followed the Starting a Work Session steps to get your Farm session set up. You should be within your nsurp-env conda environment. Install fastp: conda install -y fastp We can now trim our data! Let's set up our directory structure: cd ~/2020-NSURP mkdir -p trim cd trim Run fastp on the CSM7KOJE sample with the following command: fastp --in1 ~/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz \\ --in2 ~/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz \\ --out1 CSM7KOJE_1.trim.fastq.gz \\ --out2 CSM7KOJE_2.trim.fastq.gz \\ --detect_adapter_for_pe \\ --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json CSM7KOJE.trim.json \\ --html CSM7KOJE.trim.html Command Breakdown --in1 , --in2 - the read1 and read2 input file names --out1 , --out2 - the read1 and read2 output file names --detect_adapter_for_pe - Auto detect the adapters for our paired end (PE) reads, and remove them during trimming --length_required - discard reads shorter than length_required paramter (default is 15) --correction - enable base correction if the paired end reads overlap (only for PE data), --qualified_quality_phred - the quality value that a base is qualified. Default 15 means phred quality >=Q15 is qualified. (int [=15]) --html , --json - file name for the fastp trimming report printed to html and/or json format We change the Phred quality score cutoff to 4 to be more lenient in our trimming. Recall from our FastQC lesson that a quality score of 10 indicates a 1 in 10 chance that the base is inaccurate. A score of 20 is a 1 in 100 chance that the base is inaccurate. 30 is 1 in 1,000. And 40 in 1 in 10,000. By using a score of 4, we are more likely to keep data that has a high probability of being accurate. As done in downloading sequencing data , you can use scp to copy the html report to your computer: scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020-NSURP/trim/*.html ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the fastp trimming report. Why (or why not) do k-mer trimming? Even after quality trimming with fastp, our reads will still contain errors. Why? First, fastp trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong (and many bases will have a low Q score and still be correct)! Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because we want to retain as much coverage as possible for our downstream techniques (many of which do not suffer too much if some errors remain). An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 : The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metatranscriptomic data sets we do have the problem that we may have very low and very high coverage data. So we don\u2019t necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in our lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that\u2019s OK (and you can always trim them off if you really care). Kmer trimming with khmer Next, let's k-mer trim our data. This will take 20GB of RAM and a few hours to complete. We didn't ask for quite that much RAM when we initially got our computer, so we'll need a different one. First, exit your current srun session exit Next, use this srun command to get a larger computer that can handle the k-mer trimming analysis: srun -p bmh -J khmer -t 20:00:00 --mem=21gb -c 1 --pty bash Since we changed computers, our conda environment was automatically deactivated. Activate your project environment again: conda activate nsurp-env Install khmer We need to install the software we will use to perform k-mer trimming, khmer . Make sure you activate the conda environment you are using for this project with conda activate env_name . conda install -y khmer Using khmer for k-mer trimming Once khmer is installed, we can use it for k-mer trimming. Let's get our files and directories set up: cd ~/2020-NSURP mkdir -p kmer-trim cd kmer-trim Now we can run k-mer trimming! The first line of this command interleaves our paired end reads, putting them in one file where forward and reverse reads alternate on each line. The second line of this command performs the k-mer trimming. Note that these commands are connected by the pipe ( | ) character. This character means that the first half of the command (before the | ) is executed first, and the output is passed (\"piped\") to the second half of the command (after the | ). interleave-reads.py ~/2020-NSURP/trim/CSM7KOJE_1.trim.fastq.gz ~/2020-NSURP/trim/CSM7KOJE_2.trim.fastq.gz | \\ trim-low-abund.py --gzip -C 3 -Z 18 -M 20e9 -V - -o CSM7KOJE.kmertrim.fq.gz Note: Here, we are referencing the trimmed files using an absolute path: ~/2020-NSURP/trim/ . That is, to access these files, we go to our home directory ( ~ ), then descend into the 2020-NSURP folder, then descend again into the trim folder. Assess changes in kmer abundance To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py ../trim/CSM7KOJE_1.trim.fastq.gz ../trim/CSM7KOJE_2.trim.fastq.gz unique-kmers.py CSM7KOJE.kmertrim.fq.gz Note, here we are using a relative path, ../trim/ . That is, to access the CSM7KOJE_*.trim.fastq.gz files, we go up one directory ( ../ ), then down into trim . The raw adapter-trimmed inputs have an estimated 164426731 unique 32-mers. Estimated number of unique 32-mers in ../trim/CSM7KOJE_1.trim.fastq.gz: 83127191 Estimated number of unique 32-mers in ../trim/CSM7KOJE_2.trim.fastq.gz: 80110484 Total estimated number of unique 32-mers: 98077936 The k-mer trimmed file (kmer output) has an estimated 163890994 unique 32-mers. Estimated number of unique 32-mers in CSM7KOJE.kmertrim.fq.gz: 163890994 Total estimated number of unique 32-mers: 163890994 Note that the second number is smaller than the first, with a little over 500,000 low-abundance k-mers having been removed as likely errors. These are pretty small sample datasets that are already relatively clean - often the difference in unique k-mers is MUCH larger! Challenge: quality control Make sure you do fastp and khmer trimming on each of the 6 datasets. Keep track of the commands you use in a HackMD lab notebook. Use backticks to create code blocks and be sure to write notes describing the purpose of each step and any problems you encountered. There's no need to count unique k-mers for every dataset, but feel free if you'd like to look at the differences :).","title":"Quality Control"},{"location":"07.quality-control/#quality-control-the-data","text":"If you're starting a new work session on FARM, be sure to follow the instructions here . After downloading sequencing data , the next step in many pipelines is to perform quality control trimming on the reads. However, deciding when and how to trim data is pipeline dependent. Below, we define a few types of quality control and explore a use cases and how trimming recommendations may change with different applications. Although this project focuses on metagenomic sequencing, we include other applications in this discussion.","title":"Quality Control the Data"},{"location":"07.quality-control/#types-of-quality-control","text":"Adapter and barcode trimming : Adapter sequences are added to a sample library to aid in the physical process of sequencing. They are ubiquitous within a certain chemistry, and so are present across all sequenced samples. Barcodes are unique nucleotide sequences used to identify a specific sample when multiple samples are sequenced in a single lane. After barcoded samples are separated from one another in a process called demultiplexing, barcodes are no longer needed in a sequence. It is generally a good idea to remove adapters and barcodes from sequencing samples before proceeding with any downstream application. However, if you are using a pipeline that involves matching between reads and a quality reference, you may get similar results with or without adapter trimming. For quick estimation Quality trimming : Quality trimming removes low-quality bases from sequences reads. The user can set the stringency cut off for \"low quality\" by indicating a phred score at which to trim. K-mer trimming : K-mer trimming removes k-mers that occur very few times in a sequencing dataset. In reads with sufficient sequencing depth, we expect real k-mers to occur multiple times. When a single sequencing error occurs in a read, this produces k erroneous k-mers. K-mer trimming trims a read to remove all of these k-mers. K-mer trimming does not rely on information from the sequencer like phred scores, but instead on the biological signal in the reads themselves.","title":"Types of Quality Control"},{"location":"07.quality-control/#when-and-how-to-trim","text":"Trimming is a balance of removing artificial or incorrect nucleotides and retaining true nucleotides in sequencing data. What and when to trim therefore changes with the sequencing application, and with the sequencing data itself. Below we explore some trimming use cases to help develop an intuition for what type of trimming is necessary and when. Single-species genomic sequencing for assembly : Let's imagine we have just sequenced an Escherichia coli isolate with 100X coverage and would like to assemble the isolate. We would first want to remove adapters and barcodes to prevent these sequences from ending up in our final assembly. Then, stringent quality and k-mer trimming may be appropriate, because we have high coverage data; even if we were to stringently trim and were only left with 50% of our original number of reads, we would still have 50X coverage of very high quality data. 50X coverage is sufficient to acheive a good bacterial assembly in most cases. de novo RNA-sequencing assembly Now let's imagine we have sequenced the transcriptome of our favorite species which does not currently have a reference transcriptome. Because RNA transcripts have different abundance profiles, we can't use average coverage in the same way as we used it for single-species genomic sequencing. We need to be more careful when we k-mer and error trim so as not to accidentally remove low-abundance reads that represent true transcripts. We would likely use light quality trimming (e.g. a phred score of ~5). For k-mer trimming, we would only trim reads that contain high-abundance k-mers. Metagenome de novo assembly Trimming metagenomic reads for de novo assembly is similar to trimming RNA-sequencing reads for de novo transcriptome assembly. Because there are often low-abundance organisms that have low-coverage in our sequencing datasets, we need to be careful not to accidently remove these during trimming. Metagenome read mapping In referenced-based analyses including mapping of metagenomic reads to a set of reference genomes, reads will often map even when they contain adapters and barcodes. However, in some cases, the presence of adapters and barcodes does prevent mapping, so it is safer to remove all barcodes and adapters.","title":"When and how to trim?"},{"location":"07.quality-control/#references-about-trimming","text":"Many scientific studies have explored the trimming parameter space in an effort to make recommendations for different applications. We include some of these studies below. On the optimal trimming of high-throughput mRNA sequence data An Extensive Evaluation of Read Trimming Effects on Illumina NGS Data Analysis","title":"References about trimming"},{"location":"07.quality-control/#quality-and-adapter-trimming-with-fastp","text":"We will use fastp to do quality trimming of our reads. In the Download and Visual Assessment Module , we saw using FastQC that the Illumina Universal Adapter was present in our samples. We also saw that the sequence read quality dropped dramatically toward the end of the read. We will remove both of these sequences using fastp. Fastp also creates its own FastQC-style html reports for the files that we can look at after running.","title":"Quality and Adapter trimming with Fastp"},{"location":"07.quality-control/#run-fastp","text":"Reminder, make sure you've followed the Starting a Work Session steps to get your Farm session set up. You should be within your nsurp-env conda environment. Install fastp: conda install -y fastp We can now trim our data! Let's set up our directory structure: cd ~/2020-NSURP mkdir -p trim cd trim Run fastp on the CSM7KOJE sample with the following command: fastp --in1 ~/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz \\ --in2 ~/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz \\ --out1 CSM7KOJE_1.trim.fastq.gz \\ --out2 CSM7KOJE_2.trim.fastq.gz \\ --detect_adapter_for_pe \\ --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json CSM7KOJE.trim.json \\ --html CSM7KOJE.trim.html Command Breakdown --in1 , --in2 - the read1 and read2 input file names --out1 , --out2 - the read1 and read2 output file names --detect_adapter_for_pe - Auto detect the adapters for our paired end (PE) reads, and remove them during trimming --length_required - discard reads shorter than length_required paramter (default is 15) --correction - enable base correction if the paired end reads overlap (only for PE data), --qualified_quality_phred - the quality value that a base is qualified. Default 15 means phred quality >=Q15 is qualified. (int [=15]) --html , --json - file name for the fastp trimming report printed to html and/or json format We change the Phred quality score cutoff to 4 to be more lenient in our trimming. Recall from our FastQC lesson that a quality score of 10 indicates a 1 in 10 chance that the base is inaccurate. A score of 20 is a 1 in 100 chance that the base is inaccurate. 30 is 1 in 1,000. And 40 in 1 in 10,000. By using a score of 4, we are more likely to keep data that has a high probability of being accurate. As done in downloading sequencing data , you can use scp to copy the html report to your computer: scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020-NSURP/trim/*.html ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the fastp trimming report.","title":"Run fastp"},{"location":"07.quality-control/#why-or-why-not-do-k-mer-trimming","text":"Even after quality trimming with fastp, our reads will still contain errors. Why? First, fastp trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong (and many bases will have a low Q score and still be correct)! Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because we want to retain as much coverage as possible for our downstream techniques (many of which do not suffer too much if some errors remain). An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 : The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metatranscriptomic data sets we do have the problem that we may have very low and very high coverage data. So we don\u2019t necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in our lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that\u2019s OK (and you can always trim them off if you really care).","title":"Why (or why not) do k-mer trimming?"},{"location":"07.quality-control/#kmer-trimming-with-khmer","text":"Next, let's k-mer trim our data. This will take 20GB of RAM and a few hours to complete. We didn't ask for quite that much RAM when we initially got our computer, so we'll need a different one. First, exit your current srun session exit Next, use this srun command to get a larger computer that can handle the k-mer trimming analysis: srun -p bmh -J khmer -t 20:00:00 --mem=21gb -c 1 --pty bash Since we changed computers, our conda environment was automatically deactivated. Activate your project environment again: conda activate nsurp-env","title":"Kmer trimming with khmer"},{"location":"07.quality-control/#install-khmer","text":"We need to install the software we will use to perform k-mer trimming, khmer . Make sure you activate the conda environment you are using for this project with conda activate env_name . conda install -y khmer","title":"Install khmer"},{"location":"07.quality-control/#using-khmer-for-k-mer-trimming","text":"Once khmer is installed, we can use it for k-mer trimming. Let's get our files and directories set up: cd ~/2020-NSURP mkdir -p kmer-trim cd kmer-trim Now we can run k-mer trimming! The first line of this command interleaves our paired end reads, putting them in one file where forward and reverse reads alternate on each line. The second line of this command performs the k-mer trimming. Note that these commands are connected by the pipe ( | ) character. This character means that the first half of the command (before the | ) is executed first, and the output is passed (\"piped\") to the second half of the command (after the | ). interleave-reads.py ~/2020-NSURP/trim/CSM7KOJE_1.trim.fastq.gz ~/2020-NSURP/trim/CSM7KOJE_2.trim.fastq.gz | \\ trim-low-abund.py --gzip -C 3 -Z 18 -M 20e9 -V - -o CSM7KOJE.kmertrim.fq.gz Note: Here, we are referencing the trimmed files using an absolute path: ~/2020-NSURP/trim/ . That is, to access these files, we go to our home directory ( ~ ), then descend into the 2020-NSURP folder, then descend again into the trim folder.","title":"Using khmer for k-mer trimming"},{"location":"07.quality-control/#assess-changes-in-kmer-abundance","text":"To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py ../trim/CSM7KOJE_1.trim.fastq.gz ../trim/CSM7KOJE_2.trim.fastq.gz unique-kmers.py CSM7KOJE.kmertrim.fq.gz Note, here we are using a relative path, ../trim/ . That is, to access the CSM7KOJE_*.trim.fastq.gz files, we go up one directory ( ../ ), then down into trim . The raw adapter-trimmed inputs have an estimated 164426731 unique 32-mers. Estimated number of unique 32-mers in ../trim/CSM7KOJE_1.trim.fastq.gz: 83127191 Estimated number of unique 32-mers in ../trim/CSM7KOJE_2.trim.fastq.gz: 80110484 Total estimated number of unique 32-mers: 98077936 The k-mer trimmed file (kmer output) has an estimated 163890994 unique 32-mers. Estimated number of unique 32-mers in CSM7KOJE.kmertrim.fq.gz: 163890994 Total estimated number of unique 32-mers: 163890994 Note that the second number is smaller than the first, with a little over 500,000 low-abundance k-mers having been removed as likely errors. These are pretty small sample datasets that are already relatively clean - often the difference in unique k-mers is MUCH larger!","title":"Assess changes in kmer abundance"},{"location":"07.quality-control/#challenge-quality-control","text":"Make sure you do fastp and khmer trimming on each of the 6 datasets. Keep track of the commands you use in a HackMD lab notebook. Use backticks to create code blocks and be sure to write notes describing the purpose of each step and any problems you encountered. There's no need to count unique k-mers for every dataset, but feel free if you'd like to look at the differences :).","title":"Challenge: quality control"},{"location":"08.taxonomic-discovery-with-sourmash/","text":"Taxonomic Discovery with Sourmash Until now, we've performed general pre-processing steps on our sequencing data; sequence quality analysis and trimming usually occur at the start of any sequencing data analysis pipeline. Now we will begin performing analysis that makes sense for metagenomic sequencing data. We are working with publicly-available data, but let's pretend that this is a brand new sample that we just got back from our sequencing core. One of the first things we often want to do with new metagenome sequencing samples is figure out their approximate species composition. This allows us to tap in to all of the information known about these species and relate our community to existing literature. We can determine the approximate composition of our sample using sourmash . Introduction to sourmash Please read this tutorial for an introduction to how sourmash works. tl;dr (but actually please read it): sourmash breaks nucleotide sequences down into k-mers, systematically subsamples those k-mers into a representative \"signature\", and then enables searches for those k-mers in databases. This makes it really fast to make comparisons. Here, we will compare our metagenome sample against a pre-prepared database that contains all microbial sequences in GenBank. Workspace Setup If you're starting a new work session on FARM, be sure to follow the instructions here . You can just do the part to enter a tmux session, since we'll be using a larger srun session than usual. Starting with sourmash Sourmash doesn't have a big memory or CPU footprint, and can be run on most laptops. Below is a recommended srun command to start an interactive session in which to run the srun commands. srun -p bmh -J sourmash24 -t 24:00:00 --mem=16gb -c 1 --pty bash Install sourmash Be sure you've set up conda channels properly, as in the Install Conda section conda activate nsurp-env conda install -y sourmash Next, let's create a directory in which to store our sourmash signatures cd ~/2020-NSURP mkdir -p sourmash cd sourmash What data to use? We could run sourmash with our adapter trimmed or k-mer trimmed data. In fact, doing so would make sourmash faster because there would be fewer k-mers in the sample. We are currently comparing our sample against a database of trusted DNA sequences, so any k-mers in our sample that contain adapters sequence or errors will not match to the trusted reference sequences in the database. However, even though we very lightly trimmed our reads, there is a chance that we removed a very low abundance organism that was truly present in the sample. Given this trade-off, we use often raw reads for reference data comparisons, and quality-controlled reads for all other comparisons. Generate a sourmash signature Next, let's make sourmash signatures from our reads. Remember from the Quick Insights from Sequencing Data with sourmash tutorial that a k-mer size of 21 is approximately specific at the genus level, a 31 is at the species level, and 51 at the strain level. We will calculate our signature with all three k-mer sizes so we can choose which one we want to use later. sourmash compute -o CSM7KOJE.raw.sig --merge CSM7KOJE --scaled 2000 -k 21,31,51 --track-abundance ~/2020-NSURP/raw_data/CSM7KOJE_*fastq.gz You should see output that looks like this: == This is sourmash version 3.4.1. == == Please cite Brown and Irber (2016), doi:10.21105/joss.00027. == setting num_hashes to 0 because --scaled is set computing signatures for files: /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz, /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz Computing signature for ksizes: [21, 31, 51] Computing only nucleotide (and not protein) signatures. Computing a total of 3 signature(s). Tracking abundance of input k-mers. ... reading sequences from /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz ... /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz 9704045 sequences ... reading sequences from /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz ... /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz 9704045 sequences calculated 1 signatures for 19408090 sequences taken from 2 files saved signature(s) to CSM7KOJE.raw.sig. Note: signature license is CC0. The outputs file, CSM7KOJE.raw.sig holds a representative subset of k-mers from our original sample, as well as their abundance information. The k-mers are \"hashed\", or transformed, into numbers to make selecting, storing, and looking up the k-mers more efficient. Sourmash gather sourmash gather is a method for estimating the taxonomic composition of known sequences in a metagenome. Please go read through the sourmash documentation on Breaking down metagenomic samples with gather and lca . Check out Appendix A and B in this documentation for a good overview of how sourmash gather works. Running gather on our IBD samples can give us an idea of the microbes present in each sample. gather results provide strain-level specificity to matches in its output -- e.g. all strains that match any sequences (above a threshold) in your metagenome will be reported, along with the percent of each strain that matches. This is useful both to estimate the amount of metagenome sample that is known, and to estimate the closest strain relative to the organisms in your metagenomes. Download and unzip the database: mkdir -p ~/2020-NSURP/databases/ cd ~/2020-NSURP/databases/ curl -L https://osf.io/jgu93/download -o genbank-k31.sbt.zip cd ~/2020-NSURP/sourmash Run sourmash gather First, let's run a very quick search: sourmash gather --num-results 10 CSM7KOJE.raw.sig ~/2020-NSURP/databases/genbank-k31.sbt.zip the --num-results 10 is a way of shortening the search. In this case, we ask for only the top 10 results We see an output that looks like this: == This is sourmash version 3.4.1. == == Please cite Brown and Irber (2016), doi:10.21105/joss.00027. == selecting default query k=31. loaded query: CSM7KOJE... (k=31, DNA) loaded 1 databases. overlap p_query p_match avg_abund --------- ------- ------- --------- 6.4 Mbp 9.5% 63.6% 17.4 CBWF010000001.1 Klebsiella pneumoniae... 5.3 Mbp 25.6% 90.6% 56.5 KB851045.1 Clostridium clostridioform... 5.1 Mbp 3.6% 74.4% 8.5 GG668320.1 Clostridium hathewayi DSM ... 4.4 Mbp 2.0% 83.9% 5.4 LBDB01000001.1 Vibrio parahaemolyticu... 3.2 Mbp 5.0% 80.5% 18.6 JTBP01000001.1 Proteus mirabilis stra... 4.8 Mbp 1.0% 33.6% 4.2 JRSL01000930.1 Escherichia coli strai... 2.7 Mbp 0.8% 59.5% 3.5 FUNQ01000052.1 Clostridioides diffici... 2.5 Mbp 3.5% 33.9% 16.3 CZAT01000001.1 Flavonifractor plautii... 4.9 Mbp 2.3% 45.1% 11.7 KQ087951.1 Escherichia coli strain BI... 2.2 Mbp 3.3% 64.6% 18.1 FCEY01000001.1 Clostridium sp. AT5 ge... found 10 matches total; (truncated gather because --num-results=10) the recovered matches hit 56.8% of the query The shortened search will be quite quick. The two columns to pay attention to are p_query and p_match . p_query is the percent of the metagenome sample that is (estimated to be) from the named organism. p_match is the percent of the database match that is found in the query. These metrics` are affected by both evolutionary distance and by low coverage of the organism's gene set (low sequencing coverage, or little expression). Now, let's run the full gather analysis: This will take a long time to run. Sourmash will also output a csv with all the results information that we will use later to visualize our results. sourmash gather -o CSM7KOJE_x_genbank-k31.gather.csv CSM7KOJE.raw.sig ~/2020-NSURP/databases/genbank-k31.sbt.zip When sourmash is finished running, it tells us the % of our sequence was unclassified; i.e. it doesn't match any sequence in the database. In a later module, we may use additional steps prior to gather to improve the percent of sequence in the metagenome that is classifiable. These include, for example, using bbduk to remove additional human genome k-mers or using assembly-style programs such as megahit or spacegraphcats , to build longer contiguous gene sequences. Other Methods for Taxonomic Discovery and Classification There are many tools, such as Kraken and Kaiju, that can do taxonomic classification of individual reads from metagenomes. These seem to perform well (albeit with high false positive rates) in situations where you don\u2019t necessarily have the genome sequences that are in the metagenome. Sourmash, by contrast, can estimate which known genomes are actually present, so that you can extract them and map/align to them. It seems to have a very low false positive rate and is quite sensitive to strains. Detecting contamination or incorrect data sourmash gather taxonomic discovery can help uncover contamination or errors in your sequencing samples. We recommend doing sourmash gather immediately after receiving your data from the sequencing facility. If your environmental metagenome has a tremendous amount of mouse sequence in it... maybe the sequencing facility sent you the wrong data? Challenge: sourmash gather Gather with trimmed data Above, we ran sourmash gather on our untrimmed data. 44% of the sample did not contain sequence in any GenBank assembly. A substantial proportion of this sequence could be due to k-mers with errors. Run sourmash gather again on the adapter/ k-mer trimmed data. How much less of the sequence is unclassifiable when the errors and adapters are removed? How many species are no longer detected after k-mer and error trimming? Gather at different ksizes The genbank reference databases for signatures of ksize k=21 and k=51 are available for download. k=21 cd ~/2020-NSURP/databases/ curl -L https://osf.io/dm7n4/download -o genbank-k21.sbt.zip k=51 cd ~/2020-NSURP/databases/ curl -L https://osf.io/2uvsc/download -o genbank-k51.sbt.zip How do you expect the gather results to differ for each? Why? Test Gather parameters By running sourmash gather --help , you can see all the options for the gather program. scaled The scaled option provides a chaces to downample the query to the specified scaled factor. --scaled FLOAT downsample query to the specified scaled factor Try running gather with a scaled value of 50000. How do the results change, and why? base pair threshold for matches The threshold-bp option lets you only find matches that have at least this many base pairs in common (default 50,000 bp) Increasing the threshold makes gather quicker (at the expense of losing some of the smaller matches): --threshold-bp 10000000 What happens if you run gather with the threshold above? Decreasing the threshold will take more time, but be more thorough. A threshold of 0bp does an exhaustive search for all matches --threshold-bp 0 The full gather took quite a long time on our samples, so there's no need to run this one! But do keep it in mind as a way to make sure we get absolutely all of the matches we can get using gather.","title":"Taxonomic Discovery with Sourmash"},{"location":"08.taxonomic-discovery-with-sourmash/#taxonomic-discovery-with-sourmash","text":"Until now, we've performed general pre-processing steps on our sequencing data; sequence quality analysis and trimming usually occur at the start of any sequencing data analysis pipeline. Now we will begin performing analysis that makes sense for metagenomic sequencing data. We are working with publicly-available data, but let's pretend that this is a brand new sample that we just got back from our sequencing core. One of the first things we often want to do with new metagenome sequencing samples is figure out their approximate species composition. This allows us to tap in to all of the information known about these species and relate our community to existing literature. We can determine the approximate composition of our sample using sourmash .","title":"Taxonomic Discovery with Sourmash"},{"location":"08.taxonomic-discovery-with-sourmash/#introduction-to-sourmash","text":"Please read this tutorial for an introduction to how sourmash works. tl;dr (but actually please read it): sourmash breaks nucleotide sequences down into k-mers, systematically subsamples those k-mers into a representative \"signature\", and then enables searches for those k-mers in databases. This makes it really fast to make comparisons. Here, we will compare our metagenome sample against a pre-prepared database that contains all microbial sequences in GenBank.","title":"Introduction to sourmash"},{"location":"08.taxonomic-discovery-with-sourmash/#workspace-setup","text":"If you're starting a new work session on FARM, be sure to follow the instructions here . You can just do the part to enter a tmux session, since we'll be using a larger srun session than usual.","title":"Workspace Setup"},{"location":"08.taxonomic-discovery-with-sourmash/#starting-with-sourmash","text":"Sourmash doesn't have a big memory or CPU footprint, and can be run on most laptops. Below is a recommended srun command to start an interactive session in which to run the srun commands. srun -p bmh -J sourmash24 -t 24:00:00 --mem=16gb -c 1 --pty bash","title":"Starting with sourmash"},{"location":"08.taxonomic-discovery-with-sourmash/#install-sourmash","text":"Be sure you've set up conda channels properly, as in the Install Conda section conda activate nsurp-env conda install -y sourmash Next, let's create a directory in which to store our sourmash signatures cd ~/2020-NSURP mkdir -p sourmash cd sourmash","title":"Install sourmash"},{"location":"08.taxonomic-discovery-with-sourmash/#what-data-to-use","text":"We could run sourmash with our adapter trimmed or k-mer trimmed data. In fact, doing so would make sourmash faster because there would be fewer k-mers in the sample. We are currently comparing our sample against a database of trusted DNA sequences, so any k-mers in our sample that contain adapters sequence or errors will not match to the trusted reference sequences in the database. However, even though we very lightly trimmed our reads, there is a chance that we removed a very low abundance organism that was truly present in the sample. Given this trade-off, we use often raw reads for reference data comparisons, and quality-controlled reads for all other comparisons.","title":"What data to use?"},{"location":"08.taxonomic-discovery-with-sourmash/#generate-a-sourmash-signature","text":"Next, let's make sourmash signatures from our reads. Remember from the Quick Insights from Sequencing Data with sourmash tutorial that a k-mer size of 21 is approximately specific at the genus level, a 31 is at the species level, and 51 at the strain level. We will calculate our signature with all three k-mer sizes so we can choose which one we want to use later. sourmash compute -o CSM7KOJE.raw.sig --merge CSM7KOJE --scaled 2000 -k 21,31,51 --track-abundance ~/2020-NSURP/raw_data/CSM7KOJE_*fastq.gz You should see output that looks like this: == This is sourmash version 3.4.1. == == Please cite Brown and Irber (2016), doi:10.21105/joss.00027. == setting num_hashes to 0 because --scaled is set computing signatures for files: /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz, /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz Computing signature for ksizes: [21, 31, 51] Computing only nucleotide (and not protein) signatures. Computing a total of 3 signature(s). Tracking abundance of input k-mers. ... reading sequences from /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz ... /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R1.fastq.gz 9704045 sequences ... reading sequences from /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz ... /home/ntpierce/2020-NSURP/raw_data/CSM7KOJE_R2.fastq.gz 9704045 sequences calculated 1 signatures for 19408090 sequences taken from 2 files saved signature(s) to CSM7KOJE.raw.sig. Note: signature license is CC0. The outputs file, CSM7KOJE.raw.sig holds a representative subset of k-mers from our original sample, as well as their abundance information. The k-mers are \"hashed\", or transformed, into numbers to make selecting, storing, and looking up the k-mers more efficient.","title":"Generate a sourmash signature"},{"location":"08.taxonomic-discovery-with-sourmash/#sourmash-gather","text":"sourmash gather is a method for estimating the taxonomic composition of known sequences in a metagenome. Please go read through the sourmash documentation on Breaking down metagenomic samples with gather and lca . Check out Appendix A and B in this documentation for a good overview of how sourmash gather works. Running gather on our IBD samples can give us an idea of the microbes present in each sample. gather results provide strain-level specificity to matches in its output -- e.g. all strains that match any sequences (above a threshold) in your metagenome will be reported, along with the percent of each strain that matches. This is useful both to estimate the amount of metagenome sample that is known, and to estimate the closest strain relative to the organisms in your metagenomes.","title":"Sourmash gather"},{"location":"08.taxonomic-discovery-with-sourmash/#download-and-unzip-the-database","text":"mkdir -p ~/2020-NSURP/databases/ cd ~/2020-NSURP/databases/ curl -L https://osf.io/jgu93/download -o genbank-k31.sbt.zip cd ~/2020-NSURP/sourmash","title":"Download and unzip the database:"},{"location":"08.taxonomic-discovery-with-sourmash/#run-sourmash-gather","text":"First, let's run a very quick search: sourmash gather --num-results 10 CSM7KOJE.raw.sig ~/2020-NSURP/databases/genbank-k31.sbt.zip the --num-results 10 is a way of shortening the search. In this case, we ask for only the top 10 results We see an output that looks like this: == This is sourmash version 3.4.1. == == Please cite Brown and Irber (2016), doi:10.21105/joss.00027. == selecting default query k=31. loaded query: CSM7KOJE... (k=31, DNA) loaded 1 databases. overlap p_query p_match avg_abund --------- ------- ------- --------- 6.4 Mbp 9.5% 63.6% 17.4 CBWF010000001.1 Klebsiella pneumoniae... 5.3 Mbp 25.6% 90.6% 56.5 KB851045.1 Clostridium clostridioform... 5.1 Mbp 3.6% 74.4% 8.5 GG668320.1 Clostridium hathewayi DSM ... 4.4 Mbp 2.0% 83.9% 5.4 LBDB01000001.1 Vibrio parahaemolyticu... 3.2 Mbp 5.0% 80.5% 18.6 JTBP01000001.1 Proteus mirabilis stra... 4.8 Mbp 1.0% 33.6% 4.2 JRSL01000930.1 Escherichia coli strai... 2.7 Mbp 0.8% 59.5% 3.5 FUNQ01000052.1 Clostridioides diffici... 2.5 Mbp 3.5% 33.9% 16.3 CZAT01000001.1 Flavonifractor plautii... 4.9 Mbp 2.3% 45.1% 11.7 KQ087951.1 Escherichia coli strain BI... 2.2 Mbp 3.3% 64.6% 18.1 FCEY01000001.1 Clostridium sp. AT5 ge... found 10 matches total; (truncated gather because --num-results=10) the recovered matches hit 56.8% of the query The shortened search will be quite quick. The two columns to pay attention to are p_query and p_match . p_query is the percent of the metagenome sample that is (estimated to be) from the named organism. p_match is the percent of the database match that is found in the query. These metrics` are affected by both evolutionary distance and by low coverage of the organism's gene set (low sequencing coverage, or little expression). Now, let's run the full gather analysis: This will take a long time to run. Sourmash will also output a csv with all the results information that we will use later to visualize our results. sourmash gather -o CSM7KOJE_x_genbank-k31.gather.csv CSM7KOJE.raw.sig ~/2020-NSURP/databases/genbank-k31.sbt.zip When sourmash is finished running, it tells us the % of our sequence was unclassified; i.e. it doesn't match any sequence in the database. In a later module, we may use additional steps prior to gather to improve the percent of sequence in the metagenome that is classifiable. These include, for example, using bbduk to remove additional human genome k-mers or using assembly-style programs such as megahit or spacegraphcats , to build longer contiguous gene sequences.","title":"Run sourmash gather"},{"location":"08.taxonomic-discovery-with-sourmash/#other-methods-for-taxonomic-discovery-and-classification","text":"There are many tools, such as Kraken and Kaiju, that can do taxonomic classification of individual reads from metagenomes. These seem to perform well (albeit with high false positive rates) in situations where you don\u2019t necessarily have the genome sequences that are in the metagenome. Sourmash, by contrast, can estimate which known genomes are actually present, so that you can extract them and map/align to them. It seems to have a very low false positive rate and is quite sensitive to strains.","title":"Other Methods for Taxonomic Discovery and Classification"},{"location":"08.taxonomic-discovery-with-sourmash/#detecting-contamination-or-incorrect-data","text":"sourmash gather taxonomic discovery can help uncover contamination or errors in your sequencing samples. We recommend doing sourmash gather immediately after receiving your data from the sequencing facility. If your environmental metagenome has a tremendous amount of mouse sequence in it... maybe the sequencing facility sent you the wrong data?","title":"Detecting contamination or incorrect data"},{"location":"08.taxonomic-discovery-with-sourmash/#challenge-sourmash-gather","text":"","title":"Challenge: sourmash gather"},{"location":"08.taxonomic-discovery-with-sourmash/#gather-with-trimmed-data","text":"Above, we ran sourmash gather on our untrimmed data. 44% of the sample did not contain sequence in any GenBank assembly. A substantial proportion of this sequence could be due to k-mers with errors. Run sourmash gather again on the adapter/ k-mer trimmed data. How much less of the sequence is unclassifiable when the errors and adapters are removed? How many species are no longer detected after k-mer and error trimming?","title":"Gather with trimmed data"},{"location":"08.taxonomic-discovery-with-sourmash/#gather-at-different-ksizes","text":"The genbank reference databases for signatures of ksize k=21 and k=51 are available for download. k=21 cd ~/2020-NSURP/databases/ curl -L https://osf.io/dm7n4/download -o genbank-k21.sbt.zip k=51 cd ~/2020-NSURP/databases/ curl -L https://osf.io/2uvsc/download -o genbank-k51.sbt.zip How do you expect the gather results to differ for each? Why?","title":"Gather at different ksizes"},{"location":"08.taxonomic-discovery-with-sourmash/#test-gather-parameters","text":"By running sourmash gather --help , you can see all the options for the gather program.","title":"Test Gather parameters"},{"location":"08.taxonomic-discovery-with-sourmash/#scaled","text":"The scaled option provides a chaces to downample the query to the specified scaled factor. --scaled FLOAT downsample query to the specified scaled factor Try running gather with a scaled value of 50000. How do the results change, and why?","title":"scaled"},{"location":"08.taxonomic-discovery-with-sourmash/#base-pair-threshold-for-matches","text":"The threshold-bp option lets you only find matches that have at least this many base pairs in common (default 50,000 bp) Increasing the threshold makes gather quicker (at the expense of losing some of the smaller matches): --threshold-bp 10000000 What happens if you run gather with the threshold above? Decreasing the threshold will take more time, but be more thorough. A threshold of 0bp does an exhaustive search for all matches --threshold-bp 0 The full gather took quite a long time on our samples, so there's no need to run this one! But do keep it in mind as a way to make sure we get absolutely all of the matches we can get using gather.","title":"base pair threshold for matches"},{"location":"09.comparing-samples-with-sourmash/","text":"Comparing Samples with Sourmash Many metagenomics projects are designed to assess the differences between microorganism composition between samples. There are many ways to get at this question, but we can start by using k-mer profiles of the reads to quickly compare samples using sourmash compare . Workspace Setup If you're starting a new work session on FARM, be sure to follow the instructions here . First, let's make a directory that we will be working in: cd ~/2020-NSURP mkdir -p sourmash-compare cd sourmash-compare Note: we made a directory called sourmash for the taxonomic discovery module. It helps to name files and folders with details that will help you remember what results are contained therein. How could the prior module's folder name be changed to be more informative? Calculate sourmash signatures Now we can calculate signatures for each of the files. This will take 5 or 10 minutes to run for infile in ~/2020-NSURP/kmer-trim/*.kmertrim.fq.gz do name=$(basename ${infile} .kmertrim.fq.gz) echo $name sourmash compute -k 21,31,51 --scaled 2000 --track-abundance --merge ${name} -o ${name}.kmertrim.sig ${infile} done Note: Here we used bash for loop to compute signatures on each file with a single set of commands. Go through this tutorial to learn about loops! Compare sample signatures Using these signatures, we can compare our samples. sourmash compare -k 31 -o IBD.kmertrim.compare.np --csv IBD.kmertrim.compare.csv --ignore-abundance *sig Now let's plot! Sourmash has a built in plot utility that we can take advantage of. The output is a heatmap. Visualize the comparison using sourmash plot sourmash plot --labels IBD.kmertrim.compare.np This command produces three png files: IBD.kmertrim.compare.np.hist.png IBD.kmertrim.compare.np.dendro.png IBD.kmertrim.compare.np.matrix.png As usual, these files can be downloaded to your local computer with scp scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020-NSURP/sourmash-compare/*.png ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the files are on your local computer, double click to open each file. The .matrix.png is the heatmap file, which will show the pattern of similarity between samples It should look like this: . What does this heatmap tell you about your samples? For example, does it provide any information about which samples are from IBD patients, and which are from non-IBD patients? Visualize the comparison in an MDS plot We can use this output to make a Multidimensional Scaling plot. MDS plots are commonly used in visualize similarities and differences between samples. Here the strength is we used the k-mer content of all of our reads to calculate similarity. Install the R packages ggplot2 and ggrepel Since this is conda, it will recognize that it needs to install R alongside these, and take care of that for you! Usually you'll want to be careful of which version of R you're installing, but since we're just doing this one R command, we'll be a little lax about it. conda install r-ggplot2 r-ggrepel Download an R script to make the MDS plot The script source is here if you are interested! wget https://raw.githubusercontent.com/dib-lab/2020-NSURP/master/scripts/mds_plot.R Run the R script Rscript mds_plot.R IBD.kmertrim.compare.csv IBD.kmertrim.compare.mds.pdf This outputs a file IBD.kmertrim.compare.mds.pdf . You can see that file by downloading to your computer. It should look something like this: . How do the samples cluster? How does this compare to our heatmap, generated by sourmash plot , above?","title":"Comparing Samples with Sourmash"},{"location":"09.comparing-samples-with-sourmash/#comparing-samples-with-sourmash","text":"Many metagenomics projects are designed to assess the differences between microorganism composition between samples. There are many ways to get at this question, but we can start by using k-mer profiles of the reads to quickly compare samples using sourmash compare .","title":"Comparing Samples with Sourmash"},{"location":"09.comparing-samples-with-sourmash/#workspace-setup","text":"If you're starting a new work session on FARM, be sure to follow the instructions here . First, let's make a directory that we will be working in: cd ~/2020-NSURP mkdir -p sourmash-compare cd sourmash-compare Note: we made a directory called sourmash for the taxonomic discovery module. It helps to name files and folders with details that will help you remember what results are contained therein. How could the prior module's folder name be changed to be more informative?","title":"Workspace Setup"},{"location":"09.comparing-samples-with-sourmash/#calculate-sourmash-signatures","text":"Now we can calculate signatures for each of the files. This will take 5 or 10 minutes to run for infile in ~/2020-NSURP/kmer-trim/*.kmertrim.fq.gz do name=$(basename ${infile} .kmertrim.fq.gz) echo $name sourmash compute -k 21,31,51 --scaled 2000 --track-abundance --merge ${name} -o ${name}.kmertrim.sig ${infile} done Note: Here we used bash for loop to compute signatures on each file with a single set of commands. Go through this tutorial to learn about loops!","title":"Calculate sourmash signatures"},{"location":"09.comparing-samples-with-sourmash/#compare-sample-signatures","text":"Using these signatures, we can compare our samples. sourmash compare -k 31 -o IBD.kmertrim.compare.np --csv IBD.kmertrim.compare.csv --ignore-abundance *sig Now let's plot! Sourmash has a built in plot utility that we can take advantage of. The output is a heatmap.","title":"Compare sample signatures"},{"location":"09.comparing-samples-with-sourmash/#visualize-the-comparison-using-sourmash-plot","text":"sourmash plot --labels IBD.kmertrim.compare.np This command produces three png files: IBD.kmertrim.compare.np.hist.png IBD.kmertrim.compare.np.dendro.png IBD.kmertrim.compare.np.matrix.png As usual, these files can be downloaded to your local computer with scp scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020-NSURP/sourmash-compare/*.png ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the files are on your local computer, double click to open each file. The .matrix.png is the heatmap file, which will show the pattern of similarity between samples It should look like this: . What does this heatmap tell you about your samples? For example, does it provide any information about which samples are from IBD patients, and which are from non-IBD patients?","title":"Visualize the comparison using sourmash plot"},{"location":"09.comparing-samples-with-sourmash/#visualize-the-comparison-in-an-mds-plot","text":"We can use this output to make a Multidimensional Scaling plot. MDS plots are commonly used in visualize similarities and differences between samples. Here the strength is we used the k-mer content of all of our reads to calculate similarity.","title":"Visualize the comparison in an MDS plot"},{"location":"09.comparing-samples-with-sourmash/#install-the-r-packages-ggplot2-and-ggrepel","text":"Since this is conda, it will recognize that it needs to install R alongside these, and take care of that for you! Usually you'll want to be careful of which version of R you're installing, but since we're just doing this one R command, we'll be a little lax about it. conda install r-ggplot2 r-ggrepel","title":"Install the R packages ggplot2 and ggrepel"},{"location":"09.comparing-samples-with-sourmash/#download-an-r-script-to-make-the-mds-plot","text":"The script source is here if you are interested! wget https://raw.githubusercontent.com/dib-lab/2020-NSURP/master/scripts/mds_plot.R","title":"Download an R script to make the MDS plot"},{"location":"09.comparing-samples-with-sourmash/#run-the-r-script","text":"Rscript mds_plot.R IBD.kmertrim.compare.csv IBD.kmertrim.compare.mds.pdf This outputs a file IBD.kmertrim.compare.mds.pdf . You can see that file by downloading to your computer. It should look something like this: . How do the samples cluster? How does this compare to our heatmap, generated by sourmash plot , above?","title":"Run the R script"},{"location":"10.workflows-and-repeatability/","text":"Workflows, Automation, and Repeatability For everything we have done so far, we have copied and pasted a lot of commands to accomplish what we want. This works! But can also be time consuming, and is more prone to error. We will show you next how to put all of these commands into a shell script. A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line. Writing a shell script Let's put some of our commands from the quality trimming module into one script. We'll call it run_qc.sh . The sh at the end of the tells you that this is a bash script. First, cd into the 2020-NSURP directory cd ~/2020-NSURP Now, use nano to create and edit a file called run-qc.sh nano run-qc.sh will open the file. Now add the following text: cd ~/2020-NSURP mkdir -p quality cd quality ln -s ~/2020-NSURP/raw_data/*.fastq.gz ./ printf \"I see $(ls -1 *.fastq.gz | wc -l) files here.\\n\" for infile in *_R1.fastq.gz do name=$(basename ${infile} _R1.fastq.gz) fastp --in1 ${name}_R1.fastq.gz --in2 ${name}_R2.fastq.gz --out1 ${name}_1.trim.fastq.gz --out2 ${name}_2.trim.fastq.gz --detect_adapter_for_pe \\ --qualified_quality_phred 4 --length_required 31 --correction --json ${name}.trim.json --html ${name}.trim.html done This is now a shell script that you can use to execute all of those commands in one go, including running fastp on all six samples! Exit nano and try it out! Run: cd ~/2020-NSURP bash run-qc.sh Re-running the shell script Suppose you wanted to re-run the script. How would you do that? Well, note that the quality directory is created at the top of the script, and everything is executed in that directory. So if you remove the quality directory like so, rm -rf quality The -rf here means that you'd like to remove the whole directory \"recursively\" ( r ) and that you'd like file deltion to happen without asking for permission for each file ( f ) You can then do: bash run-qc.sh Some tricks for writing shell scripts Make it executable You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/2020-NSURP/run-qc.sh at the command line. You can now run ./run-qc.sh instead of bash run-qc.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too. Automation with Workflow Systems! Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different dataset, you're going to have to change a lot of commands. You can read more about using workflow systems to streamline data-intensive biology in our preprint here . Snakemake Snakemake is one of several workflow systems that help solve these problems. If you want to learn snakemake, we recommend working through a tutorial, such as the one here . It's also worth checking out the snakemake documentation here . Here, we'll demo how to run the same steps above, but in Snakemake. First, let's install snakemake in our conda environment: conda install -y snakemake-minimal We're going to automate the same set of commands for trimming, but in snakemake. Open a file called Snakefile using nano : nano Snakefile Here is the command we would need for a single sample, CSM7KOJE rule all: input: \"quality/CSM7KOJE_1.trim.fastq.gz\", \"quality/CSM7KOJE_2.trim.fastq.gz\" rule trim_reads: input: in1=\"raw_data/CSM7KOJE_R1.fastq.gz\", in2=\"raw_data/CSM7KOJE_R2.fastq.gz\", output: out1=\"quality/CSM7KOJE_1.trim.fastq.gz\", out2=\"quality/CSM7KOJE_2.trim.fastq.gz\", json=\"quality/CSM7KOJE.fastp.json\", html=\"quality/CSM7KOJE.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" We can run it like this: cd ~/2020-NSURP snakemake -n the -n tells snakemake to run a \"dry run\" - that is, just check that the input files exist and all files specified in rule all can be created from the rules provided within the Snakefile). you should see \"Nothing to be done.\" That's because the trimmed files already exist! Let's fix that: rm quality/CSM7KOJE*.trim.fastq.gz and now, when you run snakemake , you should see the fastp being run. Yay w00t! Then if you run snakemake again, you will see that it doesn't need to do anything - all the files are \"up to date\". Running all files at once Snakemake wouldn't be very useful if it could only trim one file at a time, so let's modify the Snakefile to run more files at once: SAMPLES = [\"CSM7KOJE\", \"CSM7KOJ0\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_R1.fastq.gz\", in2=\"raw_data/{sample}_R2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Try another dryrun: snakemake -n Now actually run the workflow: snakemake -j 1 the -j 1 tells snakemake to run a single job at a time. You can increase this number if you have access to more cpu (e.g. you're in an srun session where you asked for more cpu with the -n parameter). Again, we see there's nothing to be done - the files exist! Try removing the quality trimmed files and running again. rm quality/*.trim.fastq.gz Adding an environment We've been using a conda environment throughout our modules. We can export the installed package names to a file that we can use to re-install all packages in a single step (like on a different computer). conda env export -n nsurp-env -f ~/2020-NSURP/nsurp-environment.yaml We can use this environment in our snakemake rule as well! SAMPLES = [\"CSM7KOJE\", \"CSM7KOJ0\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_R1.fastq.gz\", in2=\"raw_data/{sample}_R2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" conda: \"nsurp-environment.yaml\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Here, we just have a single environment, so it was pretty easy to just run the Snakefile while within our nsurp-env environment. Using conda environment with snakemake becomes more useful as you use more tools, because it helps to keep different tools (which likely have different software dependencies) in separate conda environments. Run snakemake with --use-conda to have snakemake use the conda environment for this step. snakemake -j 1 --use-conda Why Automate with Workflow Systems? Workflow systems contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workow. These features ensure that the steps for data analysis are minimally documented and repeatable from start to finish. When paired with proper software management, fully-contained workows are scalable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user after weeks, months, or years. Check out our workflows preprint for a guide.","title":"Automation, Workflows, and Repeatability"},{"location":"10.workflows-and-repeatability/#workflows-automation-and-repeatability","text":"For everything we have done so far, we have copied and pasted a lot of commands to accomplish what we want. This works! But can also be time consuming, and is more prone to error. We will show you next how to put all of these commands into a shell script. A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line.","title":"Workflows, Automation, and Repeatability"},{"location":"10.workflows-and-repeatability/#writing-a-shell-script","text":"Let's put some of our commands from the quality trimming module into one script. We'll call it run_qc.sh . The sh at the end of the tells you that this is a bash script. First, cd into the 2020-NSURP directory cd ~/2020-NSURP Now, use nano to create and edit a file called run-qc.sh nano run-qc.sh will open the file. Now add the following text: cd ~/2020-NSURP mkdir -p quality cd quality ln -s ~/2020-NSURP/raw_data/*.fastq.gz ./ printf \"I see $(ls -1 *.fastq.gz | wc -l) files here.\\n\" for infile in *_R1.fastq.gz do name=$(basename ${infile} _R1.fastq.gz) fastp --in1 ${name}_R1.fastq.gz --in2 ${name}_R2.fastq.gz --out1 ${name}_1.trim.fastq.gz --out2 ${name}_2.trim.fastq.gz --detect_adapter_for_pe \\ --qualified_quality_phred 4 --length_required 31 --correction --json ${name}.trim.json --html ${name}.trim.html done This is now a shell script that you can use to execute all of those commands in one go, including running fastp on all six samples! Exit nano and try it out! Run: cd ~/2020-NSURP bash run-qc.sh","title":"Writing a shell script"},{"location":"10.workflows-and-repeatability/#re-running-the-shell-script","text":"Suppose you wanted to re-run the script. How would you do that? Well, note that the quality directory is created at the top of the script, and everything is executed in that directory. So if you remove the quality directory like so, rm -rf quality The -rf here means that you'd like to remove the whole directory \"recursively\" ( r ) and that you'd like file deltion to happen without asking for permission for each file ( f ) You can then do: bash run-qc.sh","title":"Re-running the shell script"},{"location":"10.workflows-and-repeatability/#some-tricks-for-writing-shell-scripts","text":"","title":"Some tricks for writing shell scripts"},{"location":"10.workflows-and-repeatability/#make-it-executable","text":"You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/2020-NSURP/run-qc.sh at the command line. You can now run ./run-qc.sh instead of bash run-qc.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too.","title":"Make it executable"},{"location":"10.workflows-and-repeatability/#automation-with-workflow-systems","text":"Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different dataset, you're going to have to change a lot of commands. You can read more about using workflow systems to streamline data-intensive biology in our preprint here .","title":"Automation with Workflow Systems!"},{"location":"10.workflows-and-repeatability/#snakemake","text":"Snakemake is one of several workflow systems that help solve these problems. If you want to learn snakemake, we recommend working through a tutorial, such as the one here . It's also worth checking out the snakemake documentation here . Here, we'll demo how to run the same steps above, but in Snakemake. First, let's install snakemake in our conda environment: conda install -y snakemake-minimal We're going to automate the same set of commands for trimming, but in snakemake. Open a file called Snakefile using nano : nano Snakefile Here is the command we would need for a single sample, CSM7KOJE rule all: input: \"quality/CSM7KOJE_1.trim.fastq.gz\", \"quality/CSM7KOJE_2.trim.fastq.gz\" rule trim_reads: input: in1=\"raw_data/CSM7KOJE_R1.fastq.gz\", in2=\"raw_data/CSM7KOJE_R2.fastq.gz\", output: out1=\"quality/CSM7KOJE_1.trim.fastq.gz\", out2=\"quality/CSM7KOJE_2.trim.fastq.gz\", json=\"quality/CSM7KOJE.fastp.json\", html=\"quality/CSM7KOJE.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" We can run it like this: cd ~/2020-NSURP snakemake -n the -n tells snakemake to run a \"dry run\" - that is, just check that the input files exist and all files specified in rule all can be created from the rules provided within the Snakefile). you should see \"Nothing to be done.\" That's because the trimmed files already exist! Let's fix that: rm quality/CSM7KOJE*.trim.fastq.gz and now, when you run snakemake , you should see the fastp being run. Yay w00t! Then if you run snakemake again, you will see that it doesn't need to do anything - all the files are \"up to date\".","title":"Snakemake"},{"location":"10.workflows-and-repeatability/#running-all-files-at-once","text":"Snakemake wouldn't be very useful if it could only trim one file at a time, so let's modify the Snakefile to run more files at once: SAMPLES = [\"CSM7KOJE\", \"CSM7KOJ0\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_R1.fastq.gz\", in2=\"raw_data/{sample}_R2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Try another dryrun: snakemake -n Now actually run the workflow: snakemake -j 1 the -j 1 tells snakemake to run a single job at a time. You can increase this number if you have access to more cpu (e.g. you're in an srun session where you asked for more cpu with the -n parameter). Again, we see there's nothing to be done - the files exist! Try removing the quality trimmed files and running again. rm quality/*.trim.fastq.gz","title":"Running all files at once"},{"location":"10.workflows-and-repeatability/#adding-an-environment","text":"We've been using a conda environment throughout our modules. We can export the installed package names to a file that we can use to re-install all packages in a single step (like on a different computer). conda env export -n nsurp-env -f ~/2020-NSURP/nsurp-environment.yaml We can use this environment in our snakemake rule as well! SAMPLES = [\"CSM7KOJE\", \"CSM7KOJ0\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_R1.fastq.gz\", in2=\"raw_data/{sample}_R2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" conda: \"nsurp-environment.yaml\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Here, we just have a single environment, so it was pretty easy to just run the Snakefile while within our nsurp-env environment. Using conda environment with snakemake becomes more useful as you use more tools, because it helps to keep different tools (which likely have different software dependencies) in separate conda environments. Run snakemake with --use-conda to have snakemake use the conda environment for this step. snakemake -j 1 --use-conda","title":"Adding an environment"},{"location":"10.workflows-and-repeatability/#why-automate-with-workflow-systems","text":"Workflow systems contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workow. These features ensure that the steps for data analysis are minimally documented and repeatable from start to finish. When paired with proper software management, fully-contained workows are scalable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user after weeks, months, or years. Check out our workflows preprint for a guide.","title":"Why Automate with Workflow Systems?"},{"location":"11.experiment-challenge/","text":"Experiment Challenge Thus far, we've run through a set of commands with six metagenome samples. These have been from two patients, one with Crohn's disease, one without. But there's not much we can say with just two patients (other than \"they look different!\"). Now, we'll add samples from more patients and try to understand the differences between samples. Workspace Setup If you're starting a new work session on FARM, be sure to follow the instructions here . Download Additional Files Move into the raw data folder cd ~/2020-NSURP/raw_data Download the files wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSMA33S4.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSMA33R1.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSMA33R5.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/MSM6J2QP.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/MSM6J2QF.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/MSM6J2QH.tar Untar each read set tar xf HSMA33S4.tar Trim and compute sourmash signatures for these files Using your HackMD notes, run the commands for trimming (both adapter and k-mer trimming) on these samples. For reference, the Quality Control contains code for running fastp and khmer trimming; the Comparing Samples with Sourmash contains code for computing sourmash signatures. Run Sourmash Compare Run sourmash compare and sourmash plot (as in Comparing Samples with Sourmash ). What do you notice about the sourmash comparison heatmap? Which samples are more similar to each other? Can you guess which patients have Crohn's disease or no IBD by comparing them to your prior samples? How do samples from the same patient compare to samples from different patients? Assess Taxonomic Diversity Run sourmash gather with the genbank-k31 database on these new samples. Count the total number of species found in each sample. Does it differ between Crohn's disease and non-IBD patients? Look at the sample metadata What additional information can you glean from looking at the metadata (the data about the data)? As usual, let's start by creating a directory for this mkdir -p ~/2020-NSURP/metadata cd ~/2020-NSURP/metadata All information about this project can be found here . Download the metadata file here . This file contains information for the metagenomics sequencing (which we looked at), but also a number of other assessments. This file is a spreadsheet that can be opened in Google docs or viewed with less . For example, view this file with less like so: less -S hmp2_metadata.csv This is a very large file. You can get information about a specific sample by searching out the specific sample id's we used. For example: grep HSMA33S4 hmp2_metadata.csv That's still a lot of info - let's get only the info for metagenomics samples: grep metagenomics hmp2_metadata.csv | grep HSMA33S4 The formatting is still a litle ugly. Let's direct the output to a file, and then open it with less -S : grep metagenomics hmp2_metadata.csv | grep HSMA33S4 > HSMA33S4.csv less -S HSMA33S4.csv","title":"Experiment Challenge"},{"location":"11.experiment-challenge/#experiment-challenge","text":"Thus far, we've run through a set of commands with six metagenome samples. These have been from two patients, one with Crohn's disease, one without. But there's not much we can say with just two patients (other than \"they look different!\"). Now, we'll add samples from more patients and try to understand the differences between samples.","title":"Experiment Challenge"},{"location":"11.experiment-challenge/#workspace-setup","text":"If you're starting a new work session on FARM, be sure to follow the instructions here .","title":"Workspace Setup"},{"location":"11.experiment-challenge/#download-additional-files","text":"Move into the raw data folder cd ~/2020-NSURP/raw_data Download the files wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSMA33S4.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSMA33R1.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/HSMA33R5.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/MSM6J2QP.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/MSM6J2QF.tar wget https://ibdmdb.org/tunnel/static/HMP2/WGS/1818/MSM6J2QH.tar Untar each read set tar xf HSMA33S4.tar","title":"Download Additional Files"},{"location":"11.experiment-challenge/#trim-and-compute-sourmash-signatures-for-these-files","text":"Using your HackMD notes, run the commands for trimming (both adapter and k-mer trimming) on these samples. For reference, the Quality Control contains code for running fastp and khmer trimming; the Comparing Samples with Sourmash contains code for computing sourmash signatures.","title":"Trim and compute sourmash signatures for these files"},{"location":"11.experiment-challenge/#run-sourmash-compare","text":"Run sourmash compare and sourmash plot (as in Comparing Samples with Sourmash ). What do you notice about the sourmash comparison heatmap? Which samples are more similar to each other? Can you guess which patients have Crohn's disease or no IBD by comparing them to your prior samples? How do samples from the same patient compare to samples from different patients?","title":"Run Sourmash Compare"},{"location":"11.experiment-challenge/#assess-taxonomic-diversity","text":"Run sourmash gather with the genbank-k31 database on these new samples. Count the total number of species found in each sample. Does it differ between Crohn's disease and non-IBD patients?","title":"Assess Taxonomic Diversity"},{"location":"11.experiment-challenge/#look-at-the-sample-metadata","text":"What additional information can you glean from looking at the metadata (the data about the data)? As usual, let's start by creating a directory for this mkdir -p ~/2020-NSURP/metadata cd ~/2020-NSURP/metadata All information about this project can be found here . Download the metadata file here . This file contains information for the metagenomics sequencing (which we looked at), but also a number of other assessments. This file is a spreadsheet that can be opened in Google docs or viewed with less . For example, view this file with less like so: less -S hmp2_metadata.csv This is a very large file. You can get information about a specific sample by searching out the specific sample id's we used. For example: grep HSMA33S4 hmp2_metadata.csv That's still a lot of info - let's get only the info for metagenomics samples: grep metagenomics hmp2_metadata.csv | grep HSMA33S4 The formatting is still a litle ugly. Let's direct the output to a file, and then open it with less -S : grep metagenomics hmp2_metadata.csv | grep HSMA33S4 > HSMA33S4.csv less -S HSMA33S4.csv","title":"Look at the sample metadata"},{"location":"12.angus-github/","text":"Version Control with Github Learning objectives Learn about version Control Learn about Github repositories Create local repositories Backup your work online using git Setup You\u2019ll need to sign up for a free account on GitHub.com . It\u2019s as simple as signing up for any other social network. Keep the email you picked handy; we\u2019ll be referencing it again in the lesson. Git is installed on many system, but if you don't already have it, instructions to install Git for Windows, Mac or Linux can be found here . What is Github? GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere. GitHub is now the largest online storage space of collaborative works that exists in the world What Is Git? Why use something like Git? Say you and a coworker are both updating pages on the same website. You make your changes, save them, and upload them back to the website. So far, so good. The problem comes when your coworker is working on the same page as you at the same time. One of you is about to have your work overwritten and erased. A version control application like Git keeps that from happening. You and your coworker can each upload your revisions to the same page, and Git will save two copies. Later, you can merge your changes together without losing any work along the way. You can even revert to an earlier version at any time, because Git keeps a \u201csnapshot\u201d of every change ever made. Git terms Repository: A directory or storage space where your projects can live. Sometimes GitHub users shorten this to \u201crepo.\u201d It can be local to a folder on your computer, or it can be a storage space on GitHub or another online host. You can keep code files, text files, image files, you name it, inside a repository. Version Control: Basically, the purpose Git was designed to serve. When you have a Microsoft Word file, you either overwrite every saved file with a new save, or you save multiple versions. With Git, you don\u2019t have to. It keeps \u201csnapshots\u201d of every point in time in the project\u2019s history, so you can never lose or overwrite it. Commit: This is the command that gives Git its power. When you commit, you are taking a \u201csnapshot\u201d of your repository at that point in time, giving you a checkpoint to which you can reevaluate or restore your project to any previous state. Branch: How do multiple people work on a project at the same time without Git getting them confused? Usually, they \u201cbranch off\u201d of the main project with their own versions full of changes they themselves have made. After they\u2019re done, it\u2019s time to \u201cmerge\u201d that branch back with the \u201cmaster,\u201d the main directory of the project. Git-Specific Commands git init : Initializes a new Git repository. Until you run this command inside a repository or directory, it\u2019s just a regular folder. Only after you input this does it accept further Git commands. git config : Short for \u201cconfigure,\u201d this is most useful when you\u2019re setting up Git for the first time. git help : Forgot a command? Type this into the command line to bring up the 21 most common git commands. You can also be more specific and type \u201cgit help init\u201d or another term to figure out how to use and configure a specific git command. git status : Check the status of your repository. See which files are inside it, which changes still need to be committed, and which branch of the repository you\u2019re currently working on. git add : This does not add new files to your repository. Instead, it brings new files to Git\u2019s attention. After you add files, they\u2019re included in Git\u2019s \u201csnapshots\u201d of the repository. git commit : Git\u2019s most important command. After you make any sort of change, you input this in order to take a \u201csnapshot\u201d of the repository. Usually it goes git commit -m \u201cMessage here.\u201d The -m indicates that the following section of the command should be read as a message. git branch : Working with multiple collaborators and want to make changes on your own? This command will let you build a new branch, or timeline of commits, of changes and file additions that are completely your own. Your title goes after the command. If you wanted a new branch called \u201ccats,\u201d you\u2019d type git branch cats . git checkout : Literally allows you to \u201ccheck out\u201d a repository that you are not currently inside. This is a navigational command that lets you move to the repository you want to check. You can use this command as g it checkout master to look at the master branch, or git checkout cats to look at another branch. git merge : When you\u2019re done working on a branch, you can merge your changes back to the master branch, which is visible to all collaborators. git merge cats would take all the changes you made to the \u201ccats\u201d branch and add them to the master. git push : If you\u2019re working on your local computer, and want your commits to be visible online on GitHub as well, you \u201cpush\u201d the changes up to GitHub with this command. git pull : If you\u2019re working on your local computer and want the most up-to-date version of your repository to work with, you \u201cpull\u201d the changes down from GitHub with this command. Setting Up GitHub And Git For The First Time It\u2019s time to introduce yourself to Git. Type in the following code: git config --global user.name \"Your Name Here\" Next, tell it your email and make sure it\u2019s the same email you used when you signed up for a GitHub.com account git config --global user.email \"your_email@youremail.com\" Creating Your Online Repository Now that you\u2019re all set up, it\u2019s time to create a place for your project to live. Both Git and GitHub refer to this as a repository, or \u201crepo\u201d for short, a digital directory or storage space where you can access your project, its files, and all the versions of its files that Git saves. On your Github profile, click the plus button and select a \"New Repository\". Give your repository a name & fill out the necessary information for your repository to be distinct and recognizeable. Don\u2019t worry about clicking the checkbox next to \u201cInitialize this repository with a README.\u201d A Readme file is usually a text file that explains a bit about the project. But we can make our own Readme file locally for practice. Click the green \u201cCreate Repository\u201d button and you\u2019re set. You now have an online space for your project to live in. Creating Your Local Repository To begin, let's create a new directory called MyProject. mkdir ~/MyProject Then we will move into this new directory. cd ~/MyProject To create a local repository, we will first initiate a new repository for \"MyProject\" by entering the following command: git init touch is a multi-purpose command, but one of its key uses is to creat new, empty files. In our case, we will create a new file called Readme.txt. touch Readme.txt We can check the status of our new repository by using git status . git status When we want Git to track a file, we use git add followed by the file we want Git to \"see\". If we do not use git add , Git will not \"see\" this file. git add Readme.txt Lastly, to have Git track the current \"snapshot\" of our file, we enter git commit . The -m flag allows us to add a personal message with the files we are committing. In the following example, our message is \"Add Readme.txt\". Examples of other messages could include version information, changes made to a document, document descriptions, etc. git commit -m \u201cAdd Readme.txt\u201d Now Git has a \"snapshot\" of this version of Readme.txt which you can return to at any time in the future! Connect Your Local Repository To Your GitHub Repository Online This setup also makes it easy to have multiple collaborators working on the same project. Each of you can work alone on your own computers, but upload or \u201cpush\u201d your changes up to the GitHub repository when they\u2019re ready. To tell Git the address off your remote repo in Github, Type the following replacing the address of the repo with your own git remote add origin https://github.com/username/myproject.git Git now knows there\u2019s a remote repository and it\u2019s where you want your local repository changes to go. To confirm, type this to check: git remote -v Great, Git is able to connect with our remote on Github. So, let's go ahead and push our files to Github git push origin master You will be prompted for your Github username and password at this point and you can see some output like this that git is sending packets of data to your github repo and by this you will force git to back up all of your commits since the last time you pushed to be backed up online. FOR FREE! Counting objects: 3, done. Writing objects: 100% (3/3), 217 bytes | 217.00 KiB/s, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/sateeshbio5/angus_test.git * [new branch] master -> master Note: To avoid having to type your username and password each time you push/pull from your github repos, read about Secure Login here Collaborating via GitHub GitHub Issues: Issues are a great way to keep track of tasks, enhancements, and bugs for your projects. They\u2019re kind of like email\u2014except they can be shared and discussed with all. Read more about Mastering Issues on Github here GitHub Pull-Requests: Pull requests let you tell others about changes you've pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. Look at others' repositories: Hadley Wickham (ggplot2) Yihui Xie (knitr) ANGUS 2019 Host Websites & Blogs on GitHub GitHub Pages is an awesome feature that lets you host websites/blogs for you and your projects. Hosted directly from your GitHub repository. Just edit, push, and your changes are live. Read more about GitHub Pages here Sources for this tutorial & Additional Git Resources Introductory tutorial by Lauren Orsini here Pro Git Try Git Github Guides Github Reference Git - Simple Guide Github Hello World","title":"Version Control with Git and GitHub"},{"location":"12.angus-github/#version-control-with-github","text":"Learning objectives Learn about version Control Learn about Github repositories Create local repositories Backup your work online using git","title":"Version Control with Github"},{"location":"12.angus-github/#setup","text":"You\u2019ll need to sign up for a free account on GitHub.com . It\u2019s as simple as signing up for any other social network. Keep the email you picked handy; we\u2019ll be referencing it again in the lesson. Git is installed on many system, but if you don't already have it, instructions to install Git for Windows, Mac or Linux can be found here .","title":"Setup"},{"location":"12.angus-github/#what-is-github","text":"GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere. GitHub is now the largest online storage space of collaborative works that exists in the world","title":"What is Github?"},{"location":"12.angus-github/#what-is-git","text":"Why use something like Git? Say you and a coworker are both updating pages on the same website. You make your changes, save them, and upload them back to the website. So far, so good. The problem comes when your coworker is working on the same page as you at the same time. One of you is about to have your work overwritten and erased. A version control application like Git keeps that from happening. You and your coworker can each upload your revisions to the same page, and Git will save two copies. Later, you can merge your changes together without losing any work along the way. You can even revert to an earlier version at any time, because Git keeps a \u201csnapshot\u201d of every change ever made.","title":"What Is Git?"},{"location":"12.angus-github/#git-terms","text":"","title":"Git terms"},{"location":"12.angus-github/#repository","text":"A directory or storage space where your projects can live. Sometimes GitHub users shorten this to \u201crepo.\u201d It can be local to a folder on your computer, or it can be a storage space on GitHub or another online host. You can keep code files, text files, image files, you name it, inside a repository.","title":"Repository:"},{"location":"12.angus-github/#version-control","text":"Basically, the purpose Git was designed to serve. When you have a Microsoft Word file, you either overwrite every saved file with a new save, or you save multiple versions. With Git, you don\u2019t have to. It keeps \u201csnapshots\u201d of every point in time in the project\u2019s history, so you can never lose or overwrite it.","title":"Version Control:"},{"location":"12.angus-github/#commit","text":"This is the command that gives Git its power. When you commit, you are taking a \u201csnapshot\u201d of your repository at that point in time, giving you a checkpoint to which you can reevaluate or restore your project to any previous state.","title":"Commit:"},{"location":"12.angus-github/#branch","text":"How do multiple people work on a project at the same time without Git getting them confused? Usually, they \u201cbranch off\u201d of the main project with their own versions full of changes they themselves have made. After they\u2019re done, it\u2019s time to \u201cmerge\u201d that branch back with the \u201cmaster,\u201d the main directory of the project.","title":"Branch:"},{"location":"12.angus-github/#git-specific-commands","text":"git init : Initializes a new Git repository. Until you run this command inside a repository or directory, it\u2019s just a regular folder. Only after you input this does it accept further Git commands. git config : Short for \u201cconfigure,\u201d this is most useful when you\u2019re setting up Git for the first time. git help : Forgot a command? Type this into the command line to bring up the 21 most common git commands. You can also be more specific and type \u201cgit help init\u201d or another term to figure out how to use and configure a specific git command. git status : Check the status of your repository. See which files are inside it, which changes still need to be committed, and which branch of the repository you\u2019re currently working on. git add : This does not add new files to your repository. Instead, it brings new files to Git\u2019s attention. After you add files, they\u2019re included in Git\u2019s \u201csnapshots\u201d of the repository. git commit : Git\u2019s most important command. After you make any sort of change, you input this in order to take a \u201csnapshot\u201d of the repository. Usually it goes git commit -m \u201cMessage here.\u201d The -m indicates that the following section of the command should be read as a message. git branch : Working with multiple collaborators and want to make changes on your own? This command will let you build a new branch, or timeline of commits, of changes and file additions that are completely your own. Your title goes after the command. If you wanted a new branch called \u201ccats,\u201d you\u2019d type git branch cats . git checkout : Literally allows you to \u201ccheck out\u201d a repository that you are not currently inside. This is a navigational command that lets you move to the repository you want to check. You can use this command as g it checkout master to look at the master branch, or git checkout cats to look at another branch. git merge : When you\u2019re done working on a branch, you can merge your changes back to the master branch, which is visible to all collaborators. git merge cats would take all the changes you made to the \u201ccats\u201d branch and add them to the master. git push : If you\u2019re working on your local computer, and want your commits to be visible online on GitHub as well, you \u201cpush\u201d the changes up to GitHub with this command. git pull : If you\u2019re working on your local computer and want the most up-to-date version of your repository to work with, you \u201cpull\u201d the changes down from GitHub with this command.","title":"Git-Specific Commands"},{"location":"12.angus-github/#setting-up-github-and-git-for-the-first-time","text":"It\u2019s time to introduce yourself to Git. Type in the following code: git config --global user.name \"Your Name Here\" Next, tell it your email and make sure it\u2019s the same email you used when you signed up for a GitHub.com account git config --global user.email \"your_email@youremail.com\"","title":"Setting Up GitHub And Git For The First Time"},{"location":"12.angus-github/#creating-your-online-repository","text":"Now that you\u2019re all set up, it\u2019s time to create a place for your project to live. Both Git and GitHub refer to this as a repository, or \u201crepo\u201d for short, a digital directory or storage space where you can access your project, its files, and all the versions of its files that Git saves. On your Github profile, click the plus button and select a \"New Repository\". Give your repository a name & fill out the necessary information for your repository to be distinct and recognizeable. Don\u2019t worry about clicking the checkbox next to \u201cInitialize this repository with a README.\u201d A Readme file is usually a text file that explains a bit about the project. But we can make our own Readme file locally for practice. Click the green \u201cCreate Repository\u201d button and you\u2019re set. You now have an online space for your project to live in.","title":"Creating Your Online Repository"},{"location":"12.angus-github/#creating-your-local-repository","text":"To begin, let's create a new directory called MyProject. mkdir ~/MyProject Then we will move into this new directory. cd ~/MyProject To create a local repository, we will first initiate a new repository for \"MyProject\" by entering the following command: git init touch is a multi-purpose command, but one of its key uses is to creat new, empty files. In our case, we will create a new file called Readme.txt. touch Readme.txt We can check the status of our new repository by using git status . git status When we want Git to track a file, we use git add followed by the file we want Git to \"see\". If we do not use git add , Git will not \"see\" this file. git add Readme.txt Lastly, to have Git track the current \"snapshot\" of our file, we enter git commit . The -m flag allows us to add a personal message with the files we are committing. In the following example, our message is \"Add Readme.txt\". Examples of other messages could include version information, changes made to a document, document descriptions, etc. git commit -m \u201cAdd Readme.txt\u201d Now Git has a \"snapshot\" of this version of Readme.txt which you can return to at any time in the future!","title":"Creating Your Local Repository"},{"location":"12.angus-github/#connect-your-local-repository-to-your-github-repository-online","text":"This setup also makes it easy to have multiple collaborators working on the same project. Each of you can work alone on your own computers, but upload or \u201cpush\u201d your changes up to the GitHub repository when they\u2019re ready. To tell Git the address off your remote repo in Github, Type the following replacing the address of the repo with your own git remote add origin https://github.com/username/myproject.git Git now knows there\u2019s a remote repository and it\u2019s where you want your local repository changes to go. To confirm, type this to check: git remote -v Great, Git is able to connect with our remote on Github. So, let's go ahead and push our files to Github git push origin master You will be prompted for your Github username and password at this point and you can see some output like this that git is sending packets of data to your github repo and by this you will force git to back up all of your commits since the last time you pushed to be backed up online. FOR FREE! Counting objects: 3, done. Writing objects: 100% (3/3), 217 bytes | 217.00 KiB/s, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/sateeshbio5/angus_test.git * [new branch] master -> master Note: To avoid having to type your username and password each time you push/pull from your github repos, read about Secure Login here","title":"Connect Your Local Repository To Your GitHub Repository Online"},{"location":"12.angus-github/#collaborating-via-github","text":"GitHub Issues: Issues are a great way to keep track of tasks, enhancements, and bugs for your projects. They\u2019re kind of like email\u2014except they can be shared and discussed with all. Read more about Mastering Issues on Github here GitHub Pull-Requests: Pull requests let you tell others about changes you've pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. Look at others' repositories: Hadley Wickham (ggplot2) Yihui Xie (knitr) ANGUS 2019","title":"Collaborating via GitHub"},{"location":"12.angus-github/#host-websites-blogs-on-github","text":"GitHub Pages is an awesome feature that lets you host websites/blogs for you and your projects. Hosted directly from your GitHub repository. Just edit, push, and your changes are live. Read more about GitHub Pages here","title":"Host Websites &amp; Blogs on GitHub"},{"location":"12.angus-github/#sources-for-this-tutorial-additional-git-resources","text":"Introductory tutorial by Lauren Orsini here Pro Git Try Git Github Guides Github Reference Git - Simple Guide Github Hello World","title":"Sources for this tutorial &amp; Additional Git Resources"}]}